{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a0b008",
   "metadata": {},
   "source": [
    "# Tools, Hallucinations, and ReAct Agents\n",
    "\n",
    "## 1. What are *Tools* in LLMs?\n",
    "\n",
    "**Tools** are external functions or systems that an LLM can call to get *real* information or perform *real* actions.\n",
    "\n",
    "Instead of the model guessing, it can:\n",
    "- Query a database\n",
    "- Do math\n",
    "- Call an API\n",
    "- Search documents\n",
    "- Execute code\n",
    "\n",
    "Think of tools as **giving hands and eyes to a brain that can only talk**.\n",
    "\n",
    "### Example (conceptual)\n",
    "Without tools:\n",
    "> â€œWhat is 78243 Ã— 912?â€  \n",
    "LLM guesses â†’ wrong answer.\n",
    "\n",
    "With tools:\n",
    "- LLM calls a calculator\n",
    "- Returns the exact result\n",
    "\n",
    "### Common tools\n",
    "- Calculator\n",
    "- Web search\n",
    "- Vector database retriever\n",
    "- SQL / NoSQL database\n",
    "- Code execution environment\n",
    "- Custom APIs\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What Problem Do Tools Solve?\n",
    "\n",
    "### Problem 1: LLMs are not grounded in reality\n",
    "LLMs generate text based on patterns, not facts.\n",
    "\n",
    "Tools let the model:\n",
    "- Fetch **up-to-date data**\n",
    "- Access **private or domain-specific info**\n",
    "- Produce **verifiable outputs**\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 2: LLMs are bad at precision\n",
    "LLMs struggle with:\n",
    "- Long math\n",
    "- Exact counts\n",
    "- Complex logic\n",
    "- Date calculations\n",
    "\n",
    "Tools handle precision; the LLM handles reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 3: Hallucinations\n",
    "When an LLM doesnâ€™t know something, it often *makes something up*.\n",
    "\n",
    "Tools reduce hallucinations by forcing:\n",
    "- Lookups instead of guesses\n",
    "- Evidence-based responses\n",
    "- Explicit failure when data is missing\n",
    "\n",
    "---\n",
    "\n",
    "## 3. What Are Hallucinations in LLMs?\n",
    "\n",
    "A **hallucination** is when an LLM confidently produces **false or made-up information**.\n",
    "\n",
    "### Why hallucinations happen\n",
    "- LLMs predict *likely text*, not *true text*\n",
    "- No internal fact-checking\n",
    "- Training data is incomplete or outdated\n",
    "- Model is pressured to always respond\n",
    "\n",
    "### Example\n",
    "> â€œGive me a research paper by Einstein on neural networks.â€\n",
    "\n",
    "LLM hallucination:\n",
    "> â€œEinstein published â€˜Neural Computation in 1912â€™â€¦â€\n",
    "\n",
    "Reality:\n",
    "âŒ No such paper exists.\n",
    "\n",
    "---\n",
    "\n",
    "### Types of hallucinations\n",
    "1. **Factual hallucination** â€“ wrong facts\n",
    "2. **Source hallucination** â€“ fake citations or links\n",
    "3. **Logical hallucination** â€“ reasoning errors\n",
    "4. **Tool hallucination** â€“ inventing tool results\n",
    "\n",
    "---\n",
    "\n",
    "## 4. How Tools Reduce Hallucinations (But Donâ€™t Eliminate Them)\n",
    "\n",
    "Tools help by:\n",
    "- Grounding answers in external data\n",
    "- Limiting free-form guessing\n",
    "- Providing verifiable outputs\n",
    "\n",
    "But:\n",
    "- The model can still misinterpret tool results\n",
    "- Bad prompts = bad tool usage\n",
    "- Tools only help if the model is *forced* to use them\n",
    "\n",
    "Tools reduce hallucinations â€” they donâ€™t magically remove them.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. What Is a ReAct Agent?\n",
    "\n",
    "**ReAct = Reason + Act**\n",
    "\n",
    "A ReAct agent alternates between:\n",
    "1. **Reasoning** (thinking about what to do)\n",
    "2. **Acting** (calling a tool)\n",
    "3. **Observing** (reading tool output)\n",
    "4. Repeating until done\n",
    "\n",
    "Itâ€™s a loop, not a single prompt.\n",
    "\n",
    "---\n",
    "\n",
    "### ReAct Pattern (conceptual)\n",
    "\n",
    "```text\n",
    "Question\n",
    " â†“\n",
    "Thought: I need external info\n",
    " â†“\n",
    "Action: call search tool\n",
    " â†“\n",
    "Observation: search results\n",
    " â†“\n",
    "Thought: now I can answer\n",
    " â†“\n",
    "Final Answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea634176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "from langchain_groq import ChatGroq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69781c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"qwen/qwen3-32b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be528ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**The Rise and Impact of Large Language Models (LLMs)**  \\n\\nIn the rapidly evolving landscape of artificial intelligence (AI), Large Language Models (LLMs) have emerged as transformative tools, reshaping how humans interact with technology. These models, capable of generating human-like text, answering complex questions, and performing a multitude of language-based tasks, are revolutionizing industries and challenging traditional notions of communication and problem-solving. From powering chatbots to aiding scientific research, LLMs are at the forefront of AI innovation. This essay explores the development, applications, benefits, and challenges of LLMs, while reflecting on their societal implications and future trajectory.  \\n\\n**Understanding LLMs: Training and Functionality**  \\nLLMs are built on advanced neural network architectures, particularly the transformer model, which enables them to process and generate text efficiently. Trained on vast datasets comprising books, articles, websites, and other textual sources, these models learn patterns, syntax, and context to produce coherent responses. For instance, models like GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers) leverage attention mechanisms to prioritize relevant information, allowing them to understand nuanced queries and generate contextually appropriate outputs. Their training involves billions of parameters, which enhance their ability to handle diverse tasks, from writing stories to coding software.  \\n\\n**Applications Across Industries**  \\nThe versatility of LLMs has led to their integration into numerous sectors. In **customer service**, chatbots powered by LLMs provide 24/7 support, resolving inquiries with minimal human intervention. In **education**, platforms like Duolingo use LLMs to personalize language learning, adapting to individual progress. The **healthcare** industry benefits from LLMs in diagnosing conditions by analyzing patient data or summarizing medical literature for professionals. Additionally, LLMs assist in **content creation**, enabling writers, marketers, and developers to draft articles, design websites, or generate code snippets. Their multilingual capabilities also bridge communication gaps, fostering global collaboration.  \\n\\n**Benefits and Advancements**  \\nLLMs offer significant advantages, including **efficiency**, **accessibility**, and **scalability**. By automating repetitive tasks, they free up human time for creative and strategic work. For example, legal professionals use LLMs to sift through case law, while researchers employ them to analyze vast datasets. Moreover, LLMs democratize access to knowledge, providing educational resources to underserved communities. Their ability to operate in multiple languages also promotes inclusivity, breaking down barriers in international communication.  \\n\\n**Challenges and Ethical Concerns**  \\nDespite their potential, LLMs pose critical challenges. **Bias** is a persistent issue, as models often reflect the prejudices present in their training data, leading to discriminatory outputs. For instance, a model trained on biased historical texts might perpetuate gender or racial stereotypes. **Misinformation** is another concern; LLMs can generate convincing fake news or deepfakes, undermining trust in digital content. Additionally, their **environmental impact** is substantial, as training large models consumes vast amounts of energy, contributing to carbon emissions. Ethical dilemmas also arise regarding **job displacement**, as automation threatens roles in customer service, journalism, and other fields.  \\n\\n**The Future of LLMs: Innovation and Responsibility**  \\nThe future of LLMs hinges on balancing innovation with ethical responsibility. Researchers are developing techniques to mitigate bias, such as fine-tuning models with diverse datasets and implementing transparency protocols. Advances in **few-shot learning** and **energy-efficient training** aim to reduce computational costs and environmental footprints. Furthermore, integrating LLMs with other AI technologies, like computer vision and robotics, could unlock new applications in autonomous systems and smart cities. However, regulatory frameworks must evolve to address privacy, accountability, and misuse, ensuring that LLMs serve as tools for empowerment rather than exploitation.  \\n\\n**Conclusion**  \\nLarge Language Models represent a paradigm shift in AI, offering unprecedented opportunities to enhance productivity, creativity, and global connectivity. Yet, their deployment demands careful consideration of ethical, social, and environmental consequences. As LLMs continue to evolve, collaboration among technologists, policymakers, and society will be crucial to harnessing their potential responsibly. By addressing challenges proactively, we can ensure that LLMs contribute to a future where technology amplifies human capabilities while upholding equity and sustainability.', additional_kwargs={'reasoning_content': \"Okay, I need to write an essay on Large Language Models (LLMs). Let me start by understanding what exactly LLMs are. They are a type of artificial intelligence that processes and generates human-like text. I should mention their training on vast datasets, which allows them to understand and respond to a wide range of topics.\\n\\nFirst, I should outline the key points. Maybe start with an introduction explaining what LLMs are. Then discuss their development, how they work technically, their applications, benefits, challenges, and future prospects. Wait, the user might want a more structured approach. Let me check the example response they provided earlier. Oh, they structured it with an introduction, sections on training, applications, benefits, challenges, and a conclusion. I should follow a similar structure but ensure it's original.\\n\\nI need to make sure I cover the training process, like using neural networks and massive datasets. Also, mention specific models like GPT, BERT, etc. Applications could include areas like customer service, content creation, education, healthcare. Benefits might be efficiency, accessibility, multilingual support. Challenges could involve ethical issues like bias, misinformation, job displacement. Future prospects might include more advanced models, integration with other AI technologies, ethical frameworks.\\n\\nWait, I should avoid just listing points. Need to elaborate each section with examples and explanations. For instance, in the applications section, give concrete examples of how LLMs are used in different industries. Also, in challenges, discuss specific cases where LLMs have caused issues, like generating fake news or biased content.\\n\\nI should also address the technical aspects, like how LLMs use transformer architectures, attention mechanisms, and how they process input to generate output. But maybe keep it high-level since the essay is for a general audience.\\n\\nMake sure to highlight both the positive and negative impacts. Balance is important. Also, mention the ongoing research to improve LLMs, such as making them more efficient, reducing environmental impact, and enhancing security.\\n\\nI need to check for any recent developments in LLMs, like the latest models or breakthroughs. But since the example response is from 2023, maybe stick to information up to that point. Also, avoid any markdown formatting and keep the language clear and academic.\\n\\nLet me start drafting each section step by step, ensuring smooth transitions between paragraphs. Use topic sentences to introduce each section. Conclude by summarizing the key points and reflecting on the future of LLMs.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 1401, 'prompt_tokens': 14, 'total_tokens': 1415, 'completion_time': 2.747883984, 'completion_tokens_details': {'reasoning_tokens': 503}, 'prompt_time': 0.000347257, 'prompt_tokens_details': None, 'queue_time': 0.158626612, 'total_time': 2.748231241}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb76a-f7aa-7d72-8f29-470d5363d638-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 14, 'output_tokens': 1401, 'total_tokens': 1415, 'output_token_details': {'reasoning': 503}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke('write an essay on llms')\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e258712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World Decorated\n"
     ]
    }
   ],
   "source": [
    "def decor(func):\n",
    "    val = func()\n",
    "    return val + \" Decorated\"\n",
    "\n",
    "@decor\n",
    "def get_me():\n",
    "    return \"Hello World\"\n",
    "\n",
    "\n",
    "print(get_me)#when get_me it calls decor(get_me) and returns val + \" Decorated\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5756bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World Decorated\n"
     ]
    }
   ],
   "source": [
    "def decor(func):\n",
    "    def wrapper():\n",
    "        val = func()\n",
    "        return val + \" Decorated\"\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@decor\n",
    "def get_me():\n",
    "    return \"Hello World\"\n",
    "\n",
    "print(get_me())#when get_me() it calls decor(get_me) and returns wrapper() and that means it calls wrapper() and returns val + \" Decorated\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7f4312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function process in module __main__:\n",
      "\n",
      "process(data)\n",
      "    Process input data and return cleaned output.\n",
      "\n",
      "    Args:\n",
      "        data (list): Raw input values\n",
      "\n",
      "    Returns:\n",
      "        list: Cleaned values\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process(data):\n",
    "    \"\"\"\n",
    "    Process input data and return cleaned output.\n",
    "\n",
    "    Args:\n",
    "        data (list): Raw input values\n",
    "\n",
    "    Returns:\n",
    "        list: Cleaned values\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "help(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "791eb1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'reasoning_content': 'Okay, the user is asking for the weather in New York. Let me check the tools available. There\\'s a function called get_weather that takes a location parameter. The required parameter is location, which should be a string. Since the user specified New York, I need to call get_weather with location set to \"New York\". I\\'ll make sure the JSON is correctly formatted with the name of the function and the arguments as specified.\\n', 'tool_calls': [{'id': 'jh4vhy6nv', 'function': {'arguments': '{\"location\":\"New York\"}', 'name': 'get_weather'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 112, 'prompt_tokens': 155, 'total_tokens': 267, 'completion_time': 0.154009134, 'completion_tokens_details': {'reasoning_tokens': 87}, 'prompt_time': 0.006168125, 'prompt_tokens_details': None, 'queue_time': 0.051201375, 'total_time': 0.160177259}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019bb76b-039b-7420-9df9-1f6984e20192-0' tool_calls=[{'name': 'get_weather', 'args': {'location': 'New York'}, 'id': 'jh4vhy6nv', 'type': 'tool_call'}] invalid_tool_calls=[] usage_metadata={'input_tokens': 155, 'output_tokens': 112, 'total_tokens': 267, 'output_token_details': {'reasoning': 87}}\n"
     ]
    }
   ],
   "source": [
    "# Tools\n",
    "#lets see what tools are and how they work and used in llm to get useful output for cases where llms alone can't get useful output\n",
    "# we will use langchain tool decorator and feed the function to the tool and let llm call the tool in case it needs to\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(location:str)->str:\n",
    "    \"\"\"Get the weather in a given location\"\"\"\n",
    "    return f\"The weather in {location} is sunny\"\n",
    "\n",
    "# now we feed this get_weather function to the tool and let llm call the tool in case it needs to\n",
    "\n",
    "model = llm.bind_tools([get_weather]) # here we give list of tools to the model\n",
    "\n",
    "msg = model.invoke(\"What is the weather in New York?\")#now it does not try to answer from data that the model is not trained on instead it search for tools\n",
    "\n",
    "# we can also use tool for other purposes like calling APIs, databases, etc thats why it allows us to make agents using tool\n",
    "\n",
    "print(msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2630fc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user is asking for the weather in New York. Let me check the tools available. There's a function called get_weather that takes a location parameter. The required parameter is location, which should be a string. Since the user specified New York, I need to call get_weather with location set to \"New York\". I'll make sure the JSON is correctly formatted with the name of the function and the arguments as specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(msg.additional_kwargs['reasoning_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f2b32f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'get_weather', 'args': {'location': 'New York'}, 'id': 'jh4vhy6nv', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "#now we need tool call loop so that we can call the tool then return the result to the LLM and llm can use it to give the final answer\n",
    "\n",
    "print(msg.tool_calls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba2da6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'reasoning_content': 'Okay, the user is asking for the weather in Kathmandu. I need to use the get_weather function. The function requires a location parameter. Kathmandu is the location here. So I should call get_weather with location set to \"Kathmandu\". Let me make sure there\\'s no typo. Everything looks good. I\\'ll format the tool call as specified.\\n', 'tool_calls': [{'id': 'sjynqhv62', 'function': {'arguments': '{\"location\":\"Kathmandu\"}', 'name': 'get_weather'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 103, 'prompt_tokens': 156, 'total_tokens': 259, 'completion_time': 0.158365477, 'completion_tokens_details': {'reasoning_tokens': 76}, 'prompt_time': 0.00622774, 'prompt_tokens_details': None, 'queue_time': 0.158972714, 'total_time': 0.164593217}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019bb76e-4610-7e31-9304-4bd2cf2d0c21-0' tool_calls=[{'name': 'get_weather', 'args': {'location': 'Kathmandu'}, 'id': 'sjynqhv62', 'type': 'tool_call'}] invalid_tool_calls=[] usage_metadata={'input_tokens': 156, 'output_tokens': 103, 'total_tokens': 259, 'output_token_details': {'reasoning': 76}}\n"
     ]
    }
   ],
   "source": [
    "#model generates tool calls\n",
    "\n",
    "msg = [{\n",
    "    'role': 'user',\n",
    "    'content': 'What is the weather in Kathmandu?'\n",
    "}]\n",
    "\n",
    "model = model.bind_tools([get_weather])\n",
    "msg_ai = model.invoke(msg)\n",
    "print(msg_ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae9f0ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is the weather in Kathmandu?'},\n",
       " AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking for the weather in Kathmandu. I need to use the get_weather function. The function requires a location parameter. Kathmandu is the location here. So I should call get_weather with location set to \"Kathmandu\". Let me make sure there\\'s no typo. Everything looks good. I\\'ll format the tool call as specified.\\n', 'tool_calls': [{'id': 'sjynqhv62', 'function': {'arguments': '{\"location\":\"Kathmandu\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 103, 'prompt_tokens': 156, 'total_tokens': 259, 'completion_time': 0.158365477, 'completion_tokens_details': {'reasoning_tokens': 76}, 'prompt_time': 0.00622774, 'prompt_tokens_details': None, 'queue_time': 0.158972714, 'total_time': 0.164593217}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb76e-4610-7e31-9304-4bd2cf2d0c21-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'Kathmandu'}, 'id': 'sjynqhv62', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 156, 'output_tokens': 103, 'total_tokens': 259, 'output_token_details': {'reasoning': 76}})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg.append(msg_ai)\n",
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25eff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The weather in Kathmandu is currently sunny. A perfect day to enjoy outdoor activities! ðŸ˜Š' additional_kwargs={'reasoning_content': \"Okay, the user asked about the weather in Kathmandu. I called the get_weather function with location set to Kathmandu. The response came back saying it's sunny. Now I need to present this information clearly. Let me check if there's any additional details I should mention. The function only provided the weather condition, so I'll stick to that. Maybe add a friendly note about enjoying the weather. Keep it concise and straightforward.\\n\"} response_metadata={'token_usage': {'completion_tokens': 114, 'prompt_tokens': 213, 'total_tokens': 327, 'completion_time': 0.202650978, 'completion_tokens_details': {'reasoning_tokens': 89}, 'prompt_time': 0.013415462, 'prompt_tokens_details': None, 'queue_time': 0.058700748, 'total_time': 0.21606644}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019bb76f-c634-71b3-9a31-1df9b11a0bbd-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 213, 'output_tokens': 114, 'total_tokens': 327, 'output_token_details': {'reasoning': 89}}\n"
     ]
    }
   ],
   "source": [
    "# now we will call the tool calls\n",
    "for tool_call in msg_ai.tool_calls:\n",
    "    result = get_weather.invoke(tool_call)#execute tool \n",
    "    msg.append(result)\n",
    "\n",
    "# call model with result\n",
    "\n",
    "message_result_after_context = model.invoke(msg)#provide context to llm now\n",
    "print(message_result_after_context)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b533751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 16384, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001B5FC2D7770>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001B5FC4BC2F0>, model_name='qwen/qwen3-32b', temperature=1e-08, reasoning_format='parsed', model_kwargs={}, groq_api_key=SecretStr('**********')) kwargs={'tools': [{'type': 'function', 'function': {'name': 'get_weather', 'description': 'Get the weather in a given location', 'parameters': {'properties': {'location': {'type': 'string'}}, 'required': ['location'], 'type': 'object'}}}]} config={} config_factories=[]\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25d256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
