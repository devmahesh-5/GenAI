{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447376ab",
   "metadata": {},
   "source": [
    "## Langchain InMemorySaver can be used to save the conversation history in a memory buffer.\n",
    "- It is a simple in-memory storage for the conversation history.\n",
    "- It is useful for small-scale applications where the conversation history is not too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b717a23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\repos\\llms\\llm-from-scratch\\LangchainProject\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e86e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"qwen/qwen3-32b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c9a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model = llm,\n",
    "    checkpointer=InMemorySaver()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d15d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make session or thread to which conversation is stored\n",
    "\n",
    "config = {'configurable':{'thread_id':'test'}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e029b1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I don't have access to personal information about users, including your name, unless you choose to share it with me. If you'd like me to address you by name, feel free to tell me! ðŸ˜Š\" additional_kwargs={'reasoning_content': 'Okay, the user is asking, \"What is my name?\" Hmm, I need to figure out how to respond. Let me start by recalling the conversation history. Wait, the user just started the conversation with this question. There\\'s no prior context provided. \\n\\nSo, in this case, I don\\'t have any information about the user\\'s name from previous interactions. My role is to be helpful and honest. I should let them know that I can\\'t access personal information unless they provide it. \\n\\nI should also consider if there\\'s a technical aspect here. Maybe they\\'re testing if I can retrieve their name from some external source. But no, I can\\'t access external data or personal information about users. \\n\\nI need to make sure my response is clear and polite. Let me phrase it in a way that invites them to provide their name if they\\'re comfortable. That way, I can address them by name in future interactions if they choose to share it.\\n\\nWait, maybe they\\'re using a username or handle in the platform they\\'re on. But since I don\\'t have access to that, I can\\'t know. So the best approach is to explain that I don\\'t have that information and ask them to share it if they want me to use it.\\n\\nAlso, I should check if there\\'s any ambiguity in the question. Are they asking for their own name or mine? But the question is \"What is my name?\" so they\\'re asking about themselves. \\n\\nI should make sure not to assume any details. Just a straightforward response that I can\\'t know their name unless they tell me. Keep it friendly and open-ended.\\n'} response_metadata={'token_usage': {'completion_tokens': 377, 'prompt_tokens': 13, 'total_tokens': 390, 'completion_time': 0.873190857, 'completion_tokens_details': {'reasoning_tokens': 329}, 'prompt_time': 0.00140865, 'prompt_tokens_details': None, 'queue_time': 0.0654119, 'total_time': 0.874599507}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019bb87b-00b3-7fd2-a116-619a8abef363-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 13, 'output_tokens': 377, 'total_tokens': 390, 'output_token_details': {'reasoning': 329}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage,AIMessage\n",
    "response = agent.invoke({\n",
    "    'messages':[\n",
    "        HumanMessage(\"What is my Name?\")\n",
    "    ]\n",
    "},config=config)\n",
    "print(response['messages'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db3d1e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello, Mahesh! ðŸ˜Š  \\nYouâ€™ve already shared your name with me. How can I assist you today? Let me knowâ€”Iâ€™m here to help with anything you need!' additional_kwargs={'reasoning_content': 'Okay, the user just said \"My name is Mahesh\" again. Let me check the conversation history.\\n\\nLooking back, the user first introduced their name as Mahesh in the second message. Then, the assistant acknowledged it and used it in subsequent responses. The user asked \"What is my Name?\" three times, and each time the assistant correctly replied with Mahesh. Now, the user is repeating their name again. \\n\\nHmm, maybe the user is testing if the assistant remembers their name or wants to confirm that the name is stored. Since the assistant already knows the name, the response should be friendly and reaffirm the name while offering help. The user might not have a specific question but is just checking the assistant\\'s memory. \\n\\nI should make sure the response is warm and invites the user to ask for help. Also, since the user provided the name again, maybe they want to ensure that the assistant is paying attention. The assistant should acknowledge the name and keep the tone positive. No need to ask for the name again since it\\'s already known. Just a simple greeting and offer of assistance should suffice.\\n'} response_metadata={'token_usage': {'completion_tokens': 267, 'prompt_tokens': 227, 'total_tokens': 494, 'completion_time': 0.571010118, 'completion_tokens_details': {'reasoning_tokens': 225}, 'prompt_time': 0.010206962, 'prompt_tokens_details': None, 'queue_time': 0.050790478, 'total_time': 0.58121708}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019bb87a-bd3b-7213-86ca-94d014167af5-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 227, 'output_tokens': 267, 'total_tokens': 494, 'output_token_details': {'reasoning': 225}}\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke({\n",
    "    'messages':[\n",
    "        HumanMessage(\"My name is Mahesh\")\n",
    "    ]\n",
    "},config=config)\n",
    "print(response['messages'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f56ccea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello again, Mahesh! ðŸ˜Š  \\nYouâ€™ve already shared your name with me. How can I assist you today? Let me knowâ€”Iâ€™m here to help with anything you need!' additional_kwargs={'reasoning_content': 'Okay, the user is asking \"What is my Name?\" again. Let me check the history.\\n\\nIn the first message, the user asked the same question, and I responded by saying I don\\'t have access to their info unless they provide it. Then the user told me their name is Mahesh. I greeted them and offered help. Now, they\\'re asking again. \\n\\nHmm, maybe they want to confirm if I remember their name. Since I already acknowledged Mahesh before, I should reinforce that I do remember. But why are they asking again? Could be a test, or maybe they want to ensure the system is paying attention. \\n\\nI need to make sure my response is friendly and confirms their name. Also, maybe they want to continue the conversation. I should keep it positive and open-ended. Let me make sure to mention their name again and ask how I can help. Keep the tone cheerful with an emoji to keep it light.\\n'} response_metadata={'token_usage': {'completion_tokens': 234, 'prompt_tokens': 174, 'total_tokens': 408, 'completion_time': 0.51703646, 'completion_tokens_details': {'reasoning_tokens': 191}, 'prompt_time': 0.017117574, 'prompt_tokens_details': None, 'queue_time': 0.052807816, 'total_time': 0.534154034}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019bb87a-6e48-7870-8a4d-8820f6f00df9-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 174, 'output_tokens': 234, 'total_tokens': 408, 'output_token_details': {'reasoning': 191}}\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke({\n",
    "    'messages':[\n",
    "        HumanMessage(\"What is my Name?\")\n",
    "    ]\n",
    "},config=config)\n",
    "print(response['messages'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7eff25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
