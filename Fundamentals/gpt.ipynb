{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f465aca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in d:\\repos\\llms\\llms\\llms\\.venv\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\repos\\llms\\llms\\llms\\.venv\\lib\\site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\repos\\llms\\llms\\llms\\.venv\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\repos\\llms\\llms\\llms\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\repos\\llms\\llms\\llms\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\repos\\llms\\llms\\llms\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\repos\\llms\\llms\\llms\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55acc5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ee44077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # assert (d_out % num_heads == 0), \n",
    "        #     \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = torch.nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa6f513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec954bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feed_Forward(torch.nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            torch.nn.Linear(4*cfg['emb_dim'],cfg['emb_dim'])\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38a4f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Norm(torch.nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.eps = 6e-8\n",
    "        self.scale = torch.nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = torch.nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1,keepdim = True)\n",
    "        var = x.var(dim = -1,keepdim = True)\n",
    "        out = (x - mean)/torch.sqrt(var + self.eps)\n",
    "        out = out * self.scale + self.shift\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43d4965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(\n",
    "            d_in = cfg['emb_dim'],\n",
    "            d_out = cfg['emb_dim'],#this will be the context vector dim for each token\n",
    "            context_length= cfg['context_length'],#total no of tokens in the input sequence\n",
    "            dropout=cfg['drop_rate'],\n",
    "            num_heads= cfg['n_heads'],\n",
    "            qkv_bias=cfg['qkv_bias']\n",
    "        )\n",
    "        self.ff = Feed_Forward(cfg)\n",
    "        self.norm_1 = Layer_Norm(cfg['emb_dim'])\n",
    "        self.norm_2 = Layer_Norm(cfg['emb_dim'])\n",
    "        self.dropout_shortcut = torch.nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self,x):\n",
    "        shortcut = x\n",
    "        x = self.norm_1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        # now ff\n",
    "        shortcut = x\n",
    "        x = self.norm_2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout_shortcut(x)\n",
    "        x = x+shortcut\n",
    "\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "618d0b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_Model(torch.nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.tok_emb = torch.nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "        self.pos_emb = torch.nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
    "        self.dropout = torch.nn.Dropout(cfg['drop_rate'])\n",
    "        self.tranformer = torch.nn.Sequential(\n",
    "            *[\n",
    "               Transformer(cfg) for _ in range(self.cfg['n_layers'])\n",
    "            ]\n",
    "        )\n",
    "        self.out_norm = Layer_Norm(cfg['emb_dim'])\n",
    "        self.out_head = torch.nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)\n",
    "\n",
    "\n",
    "    def forward(self,inp_idx):\n",
    "        batch,seq_len = inp_idx.shape#batch means no of seq and seqlen means no of tokens in each seq\n",
    "        x = self.tok_emb(inp_idx)\n",
    "        x_pos = self.pos_emb(torch.arange(seq_len,device=inp_idx.device))\n",
    "        x = x+x_pos\n",
    "        x = self.dropout(x)\n",
    "        x = self.tranformer(x)\n",
    "        x = self.out_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "414c94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))#appends to the list the input chunk which is a tensor of length max_length\n",
    "            self.target_ids.append(torch.tensor(target_chunk))#output chunk start from input start + 1 with length max_length\n",
    "\n",
    "            # in another iteration the another input and output chunk will be added to the list\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0feb6c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataLoader(txt,batch_size=4, stride=256, max_length=256,Shuffle=True,drop_last=True,num_workers=0):\n",
    "    tokenizer  = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    # print(dataset[0])\n",
    "    dataLoader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,# this tells in a batch how many sequences will be there if it is 4 then it will have 4 sequences in a batch means 4 input sequences and 4 output sequences\n",
    "        shuffle=Shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataLoader\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3884882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256, # Context length is the length of the input sequence means no of tokens in each input sequence\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f80e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open ('the-verdict.txt', 'r') as f:\n",
    "#     text = f.read()\n",
    "# data_loader = create_dataLoader(text, batch_size=2, stride=256, max_length=256,Shuffle=True,drop_last=True,num_workers=0)\n",
    "\n",
    "# data_iter = iter(data_loader)#it will make a iteration of the data loader and returns a iterator\n",
    "# # to acess the data we need to use next\n",
    "\n",
    "# print(next(data_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39ce52dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479 18431\n"
     ]
    }
   ],
   "source": [
    "#Train/Validate split\n",
    "rate = 0.9\n",
    "with open ('the-verdict.txt') as file:\n",
    "    txt = file.read()\n",
    "\n",
    "train = int(0.9*len(txt))\n",
    "print(len(txt),train)\n",
    "\n",
    "train_data = txt[:train]\n",
    "validate_data = txt[train:]\n",
    "# # print(train_data)\n",
    "# print(validate_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "605ec2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "train_loader = create_dataLoader(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M['context_length'],\n",
    "    stride=GPT_CONFIG_124M['context_length']\n",
    ")\n",
    "\n",
    "validate_loader = create_dataLoader(\n",
    "    validate_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M['context_length'],\n",
    "    stride=GPT_CONFIG_124M['context_length']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "646654c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb444f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256])\n",
      "torch.Size([2, 256])\n",
      "next\n",
      "torch.Size([2, 256])\n",
      "torch.Size([2, 256])\n",
      "next\n",
      "torch.Size([2, 256])\n",
      "torch.Size([2, 256])\n",
      "next\n",
      "torch.Size([2, 256])\n",
      "torch.Size([2, 256])\n",
      "next\n",
      "torch.Size([2, 256])\n",
      "torch.Size([2, 256])\n",
      "next\n",
      "torch.Size([2, 256])\n",
      "torch.Size([2, 256])\n",
      "next\n",
      "torch.Size([2, 256])\n",
      "torch.Size([2, 256])\n",
      "next\n",
      "torch.Size([2, 256])\n",
      "torch.Size([2, 256])\n",
      "next\n",
      "torch.Size([2, 256])\n",
      "torch.Size([2, 256])\n",
      "next\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(\"next\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0d2e89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loader\n",
      "torch.Size([2, 256])\n",
      "torch.Size([2, 256])\n",
      "next\n"
     ]
    }
   ],
   "source": [
    "print(\"validation loader\")\n",
    "for x,y in validate_loader:\n",
    " \n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(\"next\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0bc1d63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(validate_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc0d4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_batch_loss(input_batch,target_batch,model,device):\n",
    "    input_batch,target_batch = input_batch.to(device),target_batch.to(device)\n",
    "    logits = model(input_batch)#for a batch there can have multiple input sequence of inputs which are sent to model class and we get input in batch_size*no_of_tokens_in_one_sequence*vocab_size\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0,1),target_batch.flatten())#target was 2*256 before flatten\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loader_loss(data_loader,model,device,num_batches=None):\n",
    "    train_loss =0 \n",
    "    if(len(data_loader)==0):\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)#it gives no of batches in a dataloader where in each batch there are multiple input and target sequences\n",
    "        # we will have avg loss of these input sequences among different loss\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i,(input_batch,target_batch) in enumerate(data_loader):\n",
    "        if i<num_batches:\n",
    "            loss = calc_batch_loss(input_batch=input_batch,target_batch=target_batch,model=model,device=device)\n",
    "            #this batch are the one from one batch which are of dim 2*256 which will later in model will be done a vector embedding of dim 768 for each token to get each batch have dimension of 2*256*768 and later inside same model it will return 2*256*50257 returned as logits and these will be performed softmax where each toke have vector for every vocabloury  and then highest probablity token is chosen as output for each input token \n",
    "            train_loss +=loss\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return train_loss/num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81301c75",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "If you have a machine with a CUDA-supported GPU, the LLM will train on the GPU without making any changes to the code.\n",
    "    \n",
    "Via the device setting, we ensure that the data is loaded onto the same device as the LLM model.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8805382c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device.\n",
      "Training loss: tensor(10.9644, device='cuda:0')\n",
      "Validation loss: tensor(10.9756, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT_Model(cfg=GPT_CONFIG_124M)\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loader_loss(train_loader, model, device)\n",
    "    val_loss = calc_loader_loss(validate_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e8d2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loader_loss(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loader_loss(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a4bf1ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]#takes only last context size tokens as input in each batch\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)# gets batch*no_tokens*vocab_size\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  #takes only the last row of input sequence\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)#appens the output token to input and iterate again to predict nexxt one\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40f5d9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We can now apply the temperature scaling and multinomial function for probabilistic\n",
    "sampling introduced in the previous section to select the next token among these 3 nonzero probability scores to generate the next token. We do this in the next section by\n",
    "modifying the text generation function.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156cd8e6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: For-loop is the same as before: Get logits, and only focus on last time step\n",
    "\n",
    "Step 2: In this new section, we filter logits with top_k sampling\n",
    "\n",
    "Step 3: This is the new section where we apply temperature scaling\n",
    "    \n",
    "Step 4: Carry out greedy next-token selection as before when temperature scaling is disabled\n",
    "\n",
    "Step 5: Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "98b72117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)#batches,no_of_token,vocab_size\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3dee9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    print(flat)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "# start_context = \"Every effort moves you\"\n",
    "# tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# token_ids = generate_text_simple(\n",
    "#     model=model,\n",
    "#     idx=text_to_token_ids(start_context, tokenizer),\n",
    "#     max_new_tokens=10,\n",
    "#     context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    "# )\n",
    "\n",
    "# print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e07e19ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2497c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre trainning the llm model \n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_batch_loss(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "        # torch.save({\n",
    "        #     \"model_state_dict\": model.state_dict(),\n",
    "        #     \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        #     }, \n",
    "        #     \"model_and_optimizer.pth\"\n",
    "        #         )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c7c9d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loader_loss(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loader_loss(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e8df3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e73ac0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.822, Val loss 9.932\n",
      "Ep 1 (Step 000005): Train loss 7.924, Val loss 8.335\n",
      "tensor([6109, 3626, 6100,  345,   11,   11,   11,   11,   11,   11,   11,   11,\n",
      "          11,   11,   11,   11,   11,   11,   13,  198,  198,  198,  198,  198,\n",
      "         198,  198,  198,  198,  198,  198,  198,  198,  198,  198,  198,  198,\n",
      "         198,  198,  198,  198,  198,  198,  198,  198,  198,  198,  198,  198,\n",
      "         198,  198,  198,  198,  198,  198], device='cuda:0')\n",
      "Every effort moves you,,,,,,,,,,,,,,.                                   \n",
      "Ep 2 (Step 000010): Train loss 6.586, Val loss 7.043\n",
      "Ep 2 (Step 000015): Train loss 5.987, Val loss 6.592\n",
      "tensor([6109, 3626, 6100,  345,   11,  262,   11,  262,  286,  262,   11,  262,\n",
      "          11,  262,   11,  262,   13,  198,    1,   11,  262,   11,  262,   11,\n",
      "          11,   11,  262,   11,  290,   11,  262,   11,   11,  262,   11,   11,\n",
      "         262,   11,  290,   11,  262,   11,  262,   11,   11,  262,   11,  262,\n",
      "          11,   11,   11,   11,   11,  286], device='cuda:0')\n",
      "Every effort moves you, the, the of the, the, the, the. \", the, the,,, the, and, the,, the,, the, and, the, the,, the, the,,,,, of\n",
      "Ep 3 (Step 000020): Train loss 15.659, Val loss 15.785\n",
      "Ep 3 (Step 000025): Train loss 5.617, Val loss 6.456\n",
      "tensor([6109, 3626, 6100,  345,   11,  290,   11,  290,  465,   13,  402,  271,\n",
      "          13,  402,   11,  290,   13,  402,   13,  314,  550,  284,  262,  284,\n",
      "         262,  438,  284,  262,  402,  271,   13,  402,   13,  314,  550,   13,\n",
      "         314,  550,  284,  262,  465,  438,  262,  438,   11,  290,   11,  290,\n",
      "         438,   13,  402,  271,   11,  290], device='cuda:0')\n",
      "Every effort moves you, and, and his. Gis. G, and. G. I had to the to the-- to the Gis. G. I had. I had to the his-- the--, and, and--. Gis, and\n",
      "Ep 4 (Step 000030): Train loss 5.024, Val loss 6.325\n",
      "Ep 4 (Step 000035): Train loss 4.712, Val loss 6.249\n",
      "tensor([6109, 3626, 6100,  345,   11,  290,  314,  550,  284,  262,  286,  262,\n",
      "        4286,  284,  262, 4286,   13,  198,    1,  438,  392,   11,  314,  550,\n",
      "         284,  502,   11,  314,  373,   11,  314,  373,   11,  314,  373,  438,\n",
      "         392,  340,  373,   11,  314,  550,  262, 4286,  284,  262,  286,  262,\n",
      "        4286,  286,  262,  286,  262,  286], device='cuda:0')\n",
      "Every effort moves you, and I had to the of the picture to the picture. \"--and, I had to me, I was, I was, I was--and it was, I had the picture to the of the picture of the of the of\n",
      "Ep 5 (Step 000040): Train loss 4.143, Val loss 6.268\n",
      "tensor([6109, 3626, 6100,  345,  760,  262,  198,  198,  198,  198,  198,  198,\n",
      "         198,  198,  198,  198,  198,  198,  198,  198,  198,  198,  198,  198,\n",
      "         198,  198,  198,  198,  198,  198,  198,  198,  198,  198,  198,  198,\n",
      "         198,  198,  198,  198,  198,  198,  198,  198,    1,  257,   11,  290,\n",
      "         465,  262,   11,  290,  465,  286], device='cuda:0')\n",
      "Every effort moves you know the                                      \" a, and his the, and his of\n",
      "Ep 6 (Step 000045): Train loss 3.555, Val loss 6.183\n",
      "Ep 6 (Step 000050): Train loss 3.156, Val loss 6.144\n",
      "tensor([6109, 3626, 6100,  345,  760,  340,  373,  407,  326,  314, 2936,  438,\n",
      "          40,  550,  262, 1109,  351,  257, 1310,  257, 1310,   25,  366, 5297,\n",
      "          11,  290,  287,  262,  198,  198,  198,  198,  198,  198,  198,  198,\n",
      "         198,  198,  198,  198,  198,  198,  198,  198,  198,  198,  198,  198,\n",
      "         198,  198,  198,  198,  198,  198], device='cuda:0')\n",
      "Every effort moves you know it was not that I felt--I had the fact with a little a little: \"Yes, and in the                          \n",
      "Ep 7 (Step 000055): Train loss 2.722, Val loss 6.134\n",
      "Ep 7 (Step 000060): Train loss 2.077, Val loss 6.176\n",
      "tensor([ 6109,  3626,  6100,   345,   760,   553,   373,   530,   286,   262,\n",
      "         4286,   329,   257,  8212,   326,    11,   319,   262,   938,  1573,\n",
      "           13,   198,   198,   198,   198,   198,   198,   198,   198,   198,\n",
      "          198,   198,   198,   198,   198,   198,   198,   198,   198,   198,\n",
      "            1,   438,   392,   314,    11,   262, 50085,    13,   366,  1858,\n",
      "          547,    11,   314,   373], device='cuda:0')\n",
      "Every effort moves you know,\" was one of the picture for a smile that, on the last word.                   \"--and I, the donkey. \"There were, I was\n",
      "Ep 8 (Step 000065): Train loss 1.714, Val loss 6.216\n",
      "Ep 8 (Step 000070): Train loss 1.323, Val loss 6.170\n",
      "tensor([ 6109,  3626,  6100,   345,  1701,   198,     1,    40,  1422,   470,\n",
      "          345,   760,   340,   438,    40,   550,   257,   465,   938,  1573,\n",
      "           13,   198,     1,   438,    40,  3114,   510,    11,   314,   550,\n",
      "          587,   284,   262,  3359,   286,   465,  5986,    13,   198,     1,\n",
      "           40,   550,   587,   465,   898,   262, 50085,    13,   366,  1858,\n",
      "          547,  1528,   618,   314], device='cuda:0')\n",
      "Every effort moves you?\" \"I didn't you know it--I had a his last word. \"--I looked up, I had been to the display of his pictures. \"I had been his own the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.088, Val loss 6.277\n",
      "Ep 9 (Step 000080): Train loss 0.881, Val loss 6.323\n",
      "tensor([ 6109,  3626,  6100,   345,  1701,   198,   198,     1,  5297,   438,\n",
      "        37121,  1035, 27339,   284,   262,  1109,   351,   257,  6487,    25,\n",
      "          366,  5297,   438,   392,   416,   502,  2474,   198,   198,     1,\n",
      "         5812,    11,   287,   262,  2589,   438,   292,  3619,  2241,    11,\n",
      "          314,   550,   757,  1057,   625,   262, 50085,    13,   366,  1858,\n",
      "          547,  1528,   618,   314], device='cuda:0')\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a laugh: \"Yes--and by me!\"  \"Oh, in the moment--as Jack himself, I had again run over the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.607, Val loss 6.376\n",
      "tensor([ 6109,  3626,  6100,   345,  1701,   198,   198,     1,  5297,   438,\n",
      "        37121,  1035, 27339,   284,   262, 21296,    13,  1375,  2227,   683,\n",
      "        29178,  3474,   438,   392,   416,   502,  2474,   198,   198,  1544,\n",
      "        13818,   757,    11,   290,  9617,   736,   465,  1182,   284,   804,\n",
      "          510,   379,   262, 17548,   286,   262, 50085,    13,   366,  1858,\n",
      "          547,  1528,   618,   314], device='cuda:0')\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Training completed in 0.45 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPT_Model(GPT_CONFIG_124M)\n",
    "\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, validate_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c2f78a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.8-cp311-cp311-win_amd64.whl.metadata (52 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.61.1-cp311-cp311-win_amd64.whl.metadata (116 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in d:\\repos\\llms\\llms\\llms\\.venv\\lib\\site-packages (from matplotlib) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\repos\\llms\\llms\\llms\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\repos\\llms\\llms\\llms\\.venv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Using cached pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\repos\\llms\\llms\\llms\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\repos\\llms\\llms\\llms\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached matplotlib-3.10.8-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.3-cp311-cp311-win_amd64.whl (225 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.61.1-cp311-cp311-win_amd64.whl (2.3 MB)\n",
      "Using cached kiwisolver-1.4.9-cp311-cp311-win_amd64.whl (73 kB)\n",
      "Using cached pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   ---------------------------------------- 0/6 [pyparsing]\n",
      "   ------ --------------------------------- 1/6 [kiwisolver]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   -------------------------- ------------- 4/6 [contourpy]\n",
      "   -------------------------- ------------- 4/6 [contourpy]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   ---------------------------------------- 6/6 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pyparsing-3.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc8ea0f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m     plt.show()\n\u001b[32m     25\u001b[39m epochs_tensor = torch.linspace(\u001b[32m0\u001b[39m, num_epochs, \u001b[38;5;28mlen\u001b[39m(train_losses))\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mplot_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_seen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_losses\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mplot_losses\u001b[39m\u001b[34m(epochs_seen, tokens_seen, train_losses, val_losses)\u001b[39m\n\u001b[32m      6\u001b[39m fig, ax1 = plt.subplots(figsize=(\u001b[32m5\u001b[39m, \u001b[32m3\u001b[39m))\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Plot training and validation loss against epochs\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43max1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs_seen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTraining loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m ax1.plot(epochs_seen, val_losses, linestyle=\u001b[33m\"\u001b[39m\u001b[33m-.\u001b[39m\u001b[33m\"\u001b[39m, label=\u001b[33m\"\u001b[39m\u001b[33mValidation loss\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m ax1.set_xlabel(\u001b[33m\"\u001b[39m\u001b[33mEpochs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\repos\\llms\\LLMs\\llms\\.venv\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1777\u001b[39m, in \u001b[36mAxes.plot\u001b[39m\u001b[34m(self, scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1534\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1535\u001b[39m \u001b[33;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[32m   1536\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1774\u001b[39m \u001b[33;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1776\u001b[39m kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n\u001b[32m-> \u001b[39m\u001b[32m1777\u001b[39m lines = [*\u001b[38;5;28mself\u001b[39m._get_lines(\u001b[38;5;28mself\u001b[39m, *args, data=data, **kwargs)]\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[32m   1779\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_line(line)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\repos\\llms\\LLMs\\llms\\.venv\\Lib\\site-packages\\matplotlib\\axes\\_base.py:297\u001b[39m, in \u001b[36m_process_plot_var_args.__call__\u001b[39m\u001b[34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m     this += args[\u001b[32m0\u001b[39m],\n\u001b[32m    296\u001b[39m     args = args[\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\repos\\llms\\LLMs\\llms\\.venv\\Lib\\site-packages\\matplotlib\\axes\\_base.py:484\u001b[39m, in \u001b[36m_process_plot_var_args._plot_args\u001b[39m\u001b[34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(xy) == \u001b[32m2\u001b[39m:\n\u001b[32m    483\u001b[39m     x = _check_1d(xy[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     y = \u001b[43m_check_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    486\u001b[39m     x, y = index_of(xy[-\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\repos\\llms\\LLMs\\llms\\.venv\\Lib\\site-packages\\matplotlib\\cbook.py:1368\u001b[39m, in \u001b[36m_check_1d\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m   1362\u001b[39m \u001b[38;5;66;03m# plot requires `shape` and `ndim`.  If passed an\u001b[39;00m\n\u001b[32m   1363\u001b[39m \u001b[38;5;66;03m# object that doesn't provide them, then force to numpy array.\u001b[39;00m\n\u001b[32m   1364\u001b[39m \u001b[38;5;66;03m# Note this will strip unit information.\u001b[39;00m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m'\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1366\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m'\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1367\u001b[39m         \u001b[38;5;28mlen\u001b[39m(x.shape) < \u001b[32m1\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43matleast_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\repos\\llms\\LLMs\\llms\\.venv\\Lib\\site-packages\\numpy\\_core\\shape_base.py:63\u001b[39m, in \u001b[36matleast_1d\u001b[39m\u001b[34m(*arys)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03mConvert inputs to arrays with at least one dimension.\u001b[39;00m\n\u001b[32m     25\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arys) == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     result = \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marys\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result.ndim == \u001b[32m0\u001b[39m:\n\u001b[32m     65\u001b[39m         result = result.reshape(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\repos\\llms\\LLMs\\llms\\.venv\\Lib\\site-packages\\torch\\_tensor.py:1149\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor.__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype=dtype)\n\u001b[32m   1148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    # plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe5cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00363809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9cbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio -y\n",
    "# %pip install torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe8d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 50257])\n",
      "torch.Size([1, 5, 50257])\n",
      "torch.Size([1, 6, 50257])\n",
      "torch.Size([1, 7, 50257])\n",
      "torch.Size([1, 8, 50257])\n",
      "torch.Size([1, 9, 50257])\n",
      "torch.Size([1, 10, 50257])\n",
      "torch.Size([1, 11, 50257])\n",
      "torch.Size([1, 12, 50257])\n",
      "torch.Size([1, 13, 50257])\n",
      "torch.Size([1, 14, 50257])\n",
      "torch.Size([1, 15, 50257])\n",
      "torch.Size([1, 16, 50257])\n",
      "torch.Size([1, 17, 50257])\n",
      "torch.Size([1, 18, 50257])\n",
      "tensor([6109, 3626, 6100,  345,  772, 8812,  558,   13,  198,  198,  198,  198,\n",
      "           1,  339,   11,  530,  416,  262, 4469])\n",
      "Output text:\n",
      " Every effort moves you even terrace.\n",
      "\n",
      "\n",
      "\n",
      "\" he, one by the background\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00c2a7",
   "metadata": {},
   "source": [
    "*italicized text*<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Fortunately, saving a PyTorch model is relatively straightforward. \n",
    "\n",
    "The recommended way is to save a model's so-called state_dict, a dictionary mapping each layer to its parameters,\n",
    "using the torch.save function as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386394cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT_Model(GPT_CONFIG_124M)\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9cc5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT_Model(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (tranformer): Sequential(\n",
       "    (0): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (out_norm): Layer_Norm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT_Model(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275383d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Adaptive optimizers such as AdamW store additional parameters for each model weight.\n",
    "AdamW uses historical data to adjust learning rates for each model parameter dynamically.\n",
    "                                                   \n",
    "Without it, the optimizer resets, and the model may learn suboptimally or even fail to\n",
    "converge properly, which means that it will lose the ability to generate coherent text. \n",
    "\n",
    "Using\n",
    "torch.save, we can save both the model and optimizer state_dict contents as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf824e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT_Model(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (tranformer): Sequential(\n",
       "    (0): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): Transformer(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): Feed_Forward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm_1): Layer_Norm()\n",
       "      (norm_2): Layer_Norm()\n",
       "      (dropout_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (out_norm): Layer_Norm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can save the model and optimizer \n",
    "# best place to save model and optimizer is after each epoch in trainning so later when we again train it starts from that epoch so it becomes fast\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPT_Model(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()\n",
    "#now we can train from the checkpoint\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, validate_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a47458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading pretrainned gpt wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac4dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\repos\\llms\\LLMs\\llms\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e805a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\repos\\llms\\LLMs\\llms\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "models\\124M\\checkpoint: 100%|| 77.0/77.0 [00:00<00:00, 77.0kiB/s]\n",
      "d:\\repos\\llms\\LLMs\\llms\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "models\\124M\\encoder.json: 100%|| 1.04M/1.04M [00:03<00:00, 282kiB/s] \n",
      "d:\\repos\\llms\\LLMs\\llms\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "models\\124M\\hparams.json: 100%|| 90.0/90.0 [00:00<00:00, 89.5kiB/s]\n",
      "d:\\repos\\llms\\LLMs\\llms\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "models\\124M\\model.ckpt.data-00000-of-00001: 100%|| 498M/498M [03:13<00:00, 2.57MiB/s]   \n",
      "d:\\repos\\llms\\LLMs\\llms\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "models\\124M\\model.ckpt.index: 100%|| 5.21k/5.21k [00:00<00:00, 2.62MiB/s]\n",
      "d:\\repos\\llms\\LLMs\\llms\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "models\\124M\\model.ckpt.meta: 100%|| 471k/471k [00:01<00:00, 440kiB/s]  \n",
      "d:\\repos\\llms\\LLMs\\llms\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "models\\124M\\vocab.bpe: 100%|| 456k/456k [00:01<00:00, 423kiB/s]  \n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d1854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8994751d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c7548f",
   "metadata": {},
   "source": [
    "## this shows us that this embedding parameters are correctly loaded and has same shape as we defined now instead of randomly definning paarameters we can now use these parameter in each block and layers to get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "44736cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d22318ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "gpt = GPT_Model(NEW_CONFIG)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12e9fc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "By default, the GPTModel instance is initialized with random weights for pretraining. \n",
    "\n",
    "The last\n",
    "step to using OpenAI's model weights is to override these random weights with the weights\n",
    "we loaded into the params dictionary.\n",
    "\n",
    "For this, we will first define a small assign utility function that checks whether two\n",
    "tensors or arrays (left and right) have the same dimensions or shape and returns the\n",
    "right tensor as trainable PyTorch parameters:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "593dc597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51cd86",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: Setting the model's positional and token embedding weights to those specified in params.\n",
    "\n",
    "Step 2: Iterate over each transformer block in the model.\n",
    "\n",
    "Step 3: The np.split function is used to divide the attention and bias weights into three equal parts for the query,\n",
    "key, and value components.\n",
    "    \n",
    "Step 4: The original GPT-2 model by OpenAI reused the token embedding weights in the output layer to reduce the\n",
    "total number of parameters, which is a concept known as weight tying.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "12ef826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.tranformer[b].attn.W_query.weight = assign(\n",
    "            gpt.tranformer[b].attn.W_query.weight, q_w.T)\n",
    "        gpt.tranformer[b].attn.W_key.weight = assign(\n",
    "            gpt.tranformer[b].attn.W_key.weight, k_w.T)\n",
    "        gpt.tranformer[b].attn.W_value.weight = assign(\n",
    "            gpt.tranformer[b].attn.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.tranformer[b].attn.W_query.bias = assign(\n",
    "            gpt.tranformer[b].attn.W_query.bias, q_b)\n",
    "        gpt.tranformer[b].attn.W_key.bias = assign(\n",
    "            gpt.tranformer[b].attn.W_key.bias, k_b)\n",
    "        gpt.tranformer[b].attn.W_value.bias = assign(\n",
    "            gpt.tranformer[b].attn.W_value.bias, v_b)\n",
    "\n",
    "        gpt.tranformer[b].attn.out_proj.weight = assign(\n",
    "            gpt.tranformer[b].attn.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.tranformer[b].attn.out_proj.bias = assign(\n",
    "            gpt.tranformer[b].attn.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.tranformer[b].ff.layer[0].weight = assign(\n",
    "            gpt.tranformer[b].ff.layer[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.tranformer[b].ff.layer[0].bias = assign(\n",
    "            gpt.tranformer[b].ff.layer[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.tranformer[b].ff.layer[2].weight = assign(\n",
    "            gpt.tranformer[b].ff.layer[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.tranformer[b].ff.layer[2].bias = assign(\n",
    "            gpt.tranformer[b].ff.layer[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.tranformer[b].norm_1.scale = assign(\n",
    "            gpt.tranformer[b].norm_1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.tranformer[b].norm_1.shift = assign(\n",
    "            gpt.tranformer[b].norm_1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.tranformer[b].norm_2.scale = assign(\n",
    "            gpt.tranformer[b].norm_2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.tranformer[b].norm_2.shift = assign(\n",
    "            gpt.tranformer[b].norm_2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.out_norm.scale = assign(gpt.out_norm.scale, params[\"g\"])\n",
    "    gpt.out_norm.shift = assign(gpt.out_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff75fec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the load_weights_into_gpt function, we carefully match the weights from OpenAI's\n",
    "implementation with our GPTModel implementation. \n",
    "\n",
    "To pick a specific example, OpenAI\n",
    "stored the weight tensor for the output projection layer for the first transformer block as\n",
    "params[\"blocks\"][0][\"attn\"][\"c_proj\"][\"w\"]. \n",
    "                                                        \n",
    "In our implementation, this weight\n",
    "tensor corresponds to gpt.trf_blocks[b].att.out_proj.weight, where gpt is a\n",
    "GPTModel instance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2d400341",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62929bcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m.manual_seed(\u001b[32m123\u001b[39m)\n\u001b[32m      3\u001b[39m token_ids = generate(\n\u001b[32m      4\u001b[39m     model=gpt,\n\u001b[32m      5\u001b[39m     idx=text_to_token_ids(\u001b[33m\"\u001b[39m\u001b[33mnepal is good \u001b[39m\u001b[33m\"\u001b[39m, tokenizer).to(device),\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     temperature=\u001b[32m1.5\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOutput text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, token_ids_to_text(token_ids, tokenizer))\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"nepal is good \", tokenizer).to(device),\n",
    "    max_new_tokens=100,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "07a0f1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43fa868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
