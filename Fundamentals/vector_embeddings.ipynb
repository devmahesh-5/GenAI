{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12baa5ab",
   "metadata": {},
   "source": [
    "## Vector Embedding / Token Embedding\n",
    "Till now we learnt that each token is a scalar value. But in real life, we need to represent the text in a vector space so that there will be representation of simantic relations between tokens.\n",
    "\n",
    "- we may have used one hot encoding but this is not a good way to represent text because this does not capture semantic meaning.\n",
    "  Now, we will use vector embeddings to represent the text in a vector space.\n",
    "  it looks like this:\n",
    "  representational image below\n",
    " ![Alt text](./vector_embedding.png)\n",
    "\n",
    " But how do we get this?\n",
    " - we can train neural networks to learn vector embeddings for each token.\n",
    "   ![NN](./nn.png) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbe899b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8b5df70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.07421875e-01 -2.01171875e-01  1.23046875e-01  2.11914062e-01\n",
      " -9.13085938e-02  2.16796875e-01 -1.31835938e-01  8.30078125e-02\n",
      "  2.02148438e-01  4.78515625e-02  3.66210938e-02 -2.45361328e-02\n",
      "  2.39257812e-02 -1.60156250e-01 -2.61230469e-02  9.71679688e-02\n",
      " -6.34765625e-02  1.84570312e-01  1.70898438e-01 -1.63085938e-01\n",
      " -1.09375000e-01  1.49414062e-01 -4.65393066e-04  9.61914062e-02\n",
      "  1.68945312e-01  2.60925293e-03  8.93554688e-02  6.49414062e-02\n",
      "  3.56445312e-02 -6.93359375e-02 -1.46484375e-01 -1.21093750e-01\n",
      " -2.27539062e-01  2.45361328e-02 -1.24511719e-01 -3.18359375e-01\n",
      " -2.20703125e-01  1.30859375e-01  3.66210938e-02 -3.63769531e-02\n",
      " -1.13281250e-01  1.95312500e-01  9.76562500e-02  1.26953125e-01\n",
      "  6.59179688e-02  6.93359375e-02  1.02539062e-02  1.75781250e-01\n",
      " -1.68945312e-01  1.21307373e-03 -2.98828125e-01 -1.15234375e-01\n",
      "  5.66406250e-02 -1.77734375e-01 -2.08984375e-01  1.76757812e-01\n",
      "  2.38037109e-02 -2.57812500e-01 -4.46777344e-02  1.88476562e-01\n",
      "  5.51757812e-02  5.02929688e-02 -1.06933594e-01  1.89453125e-01\n",
      " -1.16210938e-01  8.49609375e-02 -1.71875000e-01  2.45117188e-01\n",
      " -1.73828125e-01 -8.30078125e-03  4.56542969e-02 -1.61132812e-02\n",
      "  1.86523438e-01 -6.05468750e-02 -4.17480469e-02  1.82617188e-01\n",
      "  2.20703125e-01 -1.22558594e-01 -2.55126953e-02 -3.08593750e-01\n",
      "  9.13085938e-02  1.60156250e-01  1.70898438e-01  1.19628906e-01\n",
      "  7.08007812e-02 -2.64892578e-02 -3.08837891e-02  4.06250000e-01\n",
      " -1.01562500e-01  5.71289062e-02 -7.26318359e-03 -9.17968750e-02\n",
      " -1.50390625e-01 -2.55859375e-01  2.16796875e-01 -3.63769531e-02\n",
      "  2.24609375e-01  8.00781250e-02  1.56250000e-01  5.27343750e-02\n",
      "  1.50390625e-01 -1.14746094e-01 -8.64257812e-02  1.19140625e-01\n",
      " -7.17773438e-02  2.73437500e-01 -1.64062500e-01  7.29370117e-03\n",
      "  4.21875000e-01 -1.12792969e-01 -1.35742188e-01 -1.31835938e-01\n",
      " -1.37695312e-01 -7.66601562e-02  6.25000000e-02  4.98046875e-02\n",
      " -1.91406250e-01 -6.03027344e-02  2.27539062e-01  5.88378906e-02\n",
      " -3.24218750e-01  5.41992188e-02 -1.35742188e-01  8.17871094e-03\n",
      " -5.24902344e-02 -1.74713135e-03 -9.81445312e-02 -2.86865234e-02\n",
      "  3.61328125e-02  2.15820312e-01  5.98144531e-02 -3.08593750e-01\n",
      " -2.27539062e-01  2.61718750e-01  9.86328125e-02 -5.07812500e-02\n",
      "  1.78222656e-02  1.31835938e-01 -5.35156250e-01 -1.81640625e-01\n",
      "  1.38671875e-01 -3.10546875e-01 -9.71679688e-02  1.31835938e-01\n",
      " -1.16210938e-01  7.03125000e-02  2.85156250e-01  3.51562500e-02\n",
      " -1.01562500e-01 -3.75976562e-02  1.41601562e-01  1.42578125e-01\n",
      " -5.68847656e-02  2.65625000e-01 -2.09960938e-01  9.64355469e-03\n",
      " -6.68945312e-02 -4.83398438e-02 -6.10351562e-02  2.45117188e-01\n",
      " -9.66796875e-02  1.78222656e-02 -1.27929688e-01 -4.78515625e-02\n",
      " -7.26318359e-03  1.79687500e-01  2.78320312e-02 -2.10937500e-01\n",
      " -1.43554688e-01 -1.27929688e-01  1.73339844e-02 -3.60107422e-03\n",
      " -2.04101562e-01  3.63159180e-03 -1.19628906e-01 -6.15234375e-02\n",
      "  5.93261719e-02 -3.23486328e-03 -1.70898438e-01 -3.14941406e-02\n",
      " -8.88671875e-02 -2.89062500e-01  3.44238281e-02 -1.87500000e-01\n",
      "  2.94921875e-01  1.58203125e-01 -1.19628906e-01  7.61718750e-02\n",
      "  6.39648438e-02 -4.68750000e-02 -6.83593750e-02  1.21459961e-02\n",
      " -1.44531250e-01  4.54101562e-02  3.68652344e-02  3.88671875e-01\n",
      "  1.45507812e-01 -2.55859375e-01 -4.46777344e-02 -1.33789062e-01\n",
      " -1.38671875e-01  6.59179688e-02  1.37695312e-01  1.14746094e-01\n",
      "  2.03125000e-01 -4.78515625e-02  1.80664062e-02 -8.54492188e-02\n",
      " -2.48046875e-01 -3.39843750e-01 -2.83203125e-02  1.05468750e-01\n",
      " -2.14843750e-01 -8.74023438e-02  7.12890625e-02  1.87500000e-01\n",
      " -1.12304688e-01  2.73437500e-01 -3.26171875e-01 -1.77734375e-01\n",
      " -4.24804688e-02 -2.69531250e-01  6.64062500e-02 -6.88476562e-02\n",
      " -1.99218750e-01 -7.03125000e-02 -2.43164062e-01 -3.66210938e-02\n",
      " -7.37304688e-02 -1.77734375e-01  9.17968750e-02 -1.25000000e-01\n",
      " -1.65039062e-01 -3.57421875e-01 -2.85156250e-01 -1.66992188e-01\n",
      "  1.97265625e-01 -1.53320312e-01  2.31933594e-02  2.06054688e-01\n",
      "  1.80664062e-01 -2.74658203e-02 -1.92382812e-01 -9.61914062e-02\n",
      " -1.06811523e-02 -4.73632812e-02  6.54296875e-02 -1.25732422e-02\n",
      "  1.78222656e-02 -8.00781250e-02 -2.59765625e-01  9.37500000e-02\n",
      " -7.81250000e-02  4.68750000e-02 -2.22167969e-02  1.86767578e-02\n",
      "  3.11279297e-02  1.04980469e-02 -1.69921875e-01  2.58789062e-02\n",
      " -3.41796875e-02 -1.44042969e-02 -5.46875000e-02 -8.78906250e-02\n",
      "  1.96838379e-03  2.23632812e-01 -1.36718750e-01  1.75781250e-01\n",
      " -1.63085938e-01  1.87500000e-01  3.44238281e-02 -5.63964844e-02\n",
      " -2.27689743e-05  4.27246094e-02  5.81054688e-02 -1.07910156e-01\n",
      " -3.88183594e-02 -2.69531250e-01  3.34472656e-02  9.81445312e-02\n",
      "  5.63964844e-02  2.23632812e-01 -5.49316406e-02  1.46484375e-01\n",
      "  5.93261719e-02 -2.19726562e-01  6.39648438e-02  1.66015625e-02\n",
      "  4.56542969e-02  3.26171875e-01 -3.80859375e-01  1.70898438e-01\n",
      "  5.66406250e-02 -1.04492188e-01  1.38671875e-01 -1.57226562e-01\n",
      "  3.23486328e-03 -4.80957031e-02 -2.48046875e-01 -6.20117188e-02]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load(\"word2vec-google-news-300\")  # download the model and return as object ready for use\n",
    "word_vectors=model\n",
    "\n",
    "# Let us look how the vector embedding of a word looks like\n",
    "print(word_vectors.get_vector('computer'))  # type: ignore # Example: Accessing the vector for the word 'computer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fc4ae4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.518113374710083), ('sultan', 0.5098593235015869), ('monarchy', 0.5087411403656006)]\n"
     ]
    }
   ],
   "source": [
    "# waht will we get with Woman + King - Man\n",
    "print(word_vectors.most_similar(positive=['woman', 'king'], negative=['man'], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf9697",
   "metadata": {},
   "source": [
    "## How token ids are converted to vector embeddings?\n",
    "- 1st we tokenize the text\n",
    "- 2nd we convert the tokens to token ids \n",
    "- 3rd is to convert the token ids to vector embeddings\n",
    "  - here, first define vocabloury size and the dimension of the vector embeddings\n",
    "  - eg: GPT-2 Small model has 50257 vocabloury size and 768 dimension of vector embeddings\n",
    "  - now the total there should be 50257 rows and 768 columns\n",
    "  - and 1st we assign a random vector to each token id\n",
    "  - 2nd we train the model to learn the vector embeddings for each token id\n",
    "  - boom! we get the vector embeddings for each token id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccb06f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aef569ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets do it on our own\n",
    "# lets embedd the token id from the text 'The quick brown fox jumps over the lazy dog'\n",
    "text = 'quick fox is in the house'\n",
    "#say we already have tokenids as a list for above text\n",
    "token_ids = torch.tensor([\n",
    "    4,0,3,2,5,1\n",
    "])\n",
    "dim = 3\n",
    "vocab_size = 6\n",
    "\n",
    "torch.manual_seed(0)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, dim)#similar to nn.linear_layer but embeding is computationally more efficient as it does not use unneccessary matrix multiplications with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2aa2e193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1258, -1.1524,  0.5667],\n",
      "        [ 0.7935,  0.5988, -1.5551],\n",
      "        [-0.3414,  1.8530,  0.4681],\n",
      "        [-0.1577, -0.1734,  0.1835],\n",
      "        [ 1.3894,  1.5863,  0.9463],\n",
      "        [-0.8437,  0.9318,  1.2590]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)##this is our embedding matrix at initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f10025a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7935,  0.5988, -1.5551]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#lets get vector for token id 1 which is second row of embedding matrix or lookup table\n",
    "\n",
    "print(embedding_layer(torch.tensor([1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0935350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3894,  1.5863,  0.9463],\n",
      "        [-1.1258, -1.1524,  0.5667],\n",
      "        [-0.1577, -0.1734,  0.1835],\n",
      "        [-0.3414,  1.8530,  0.4681],\n",
      "        [-0.8437,  0.9318,  1.2590],\n",
      "        [ 0.7935,  0.5988, -1.5551]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# for all input ids\n",
    "print(embedding_layer(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0e3c2",
   "metadata": {},
   "source": [
    "## positional Embedding\n",
    "in the Vector encoding the encoding vector for a token is always same regardless of the position of the token in the text.\n",
    "- this leads to the situation that the 2 different sentences 'the cat sat on the mat' and 'the mat sat on the cat' will have the same encoding vector.\n",
    "- to avoid this, we can add positional embeddings to the vector encoding.\n",
    "  there are 2 types of Positional Embeddings\n",
    "  1. Absolute Positional Embeddings\n",
    "   - this is the most used type of positional embeddings.\n",
    "   - in this,the dimension of the positional embedding is equal to the dimension of the vector embedding\n",
    "   - there is position embedding + token embedding = input embedding\n",
    "   - they are used when fixed order of token is crucial such as sequence generation.\n",
    "   - open ai gpt model uses absolute positional embeddings\n",
    "  ![Alt text](./pe.png)\n",
    "  2. Relative Positional Embeddings\n",
    "   - this is the least used type of positional embeddings.\n",
    "   - it focuses on how far the relative words are\n",
    "   - these are important for longer sequence where the same phrase may occur multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4b0729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.1258, -1.1524, -0.2506,  ...,  0.1447,  1.9029,  0.3904],\n",
      "        [-0.0394, -0.8015, -0.4955,  ..., -1.6989,  1.3094, -1.6613],\n",
      "        [-0.5461, -0.6302, -0.6347,  ...,  1.6553,  0.5204, -0.2326],\n",
      "        ...,\n",
      "        [-1.2348, -0.9181,  0.7427,  ..., -0.2906, -0.0758, -0.4074],\n",
      "        [ 1.7858,  0.3888,  0.8426,  ...,  0.5866, -0.2532,  0.2780],\n",
      "        [-0.3543,  0.7362,  0.5639,  ..., -0.9902,  0.7611, -0.8797]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# lets do a demo of absolute positional embeddings\n",
    "vocab_size = 50257\n",
    "dim = 256\n",
    "\n",
    "torch.manual_seed(0)\n",
    "embedding_layer_vec = torch.nn.Embedding(vocab_size, dim)\n",
    "print(embedding_layer_vec.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1274571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a8bcd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c7c8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f3eeff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataLoader(txt,batch_size=4, stride=128, max_length=256,Shuffle=True,drop_last=True,num_workers=0,tokenizer= tiktoken.get_encoding(\"gpt2\")):\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    # print(dataset[0])\n",
    "    dataLoader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=Shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataLoader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01bf4dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('the-verdict.txt', 'r') as f:\n",
    "    txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34cca11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape torch.Size([8, 4])\n",
      "output shape torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "tokenizer  = tiktoken.get_encoding(\"gpt2\")\n",
    "data_loader = create_dataLoader(txt,tokenizer=tokenizer,max_length=4,stride=4,batch_size=8)#[tensor([[][][][]]),tensor([[],[],[],[]])],[tensor([],[],[],[]),tensor([],[],[],[])],....]\n",
    "\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "input, output = next(data_iter)\n",
    "# print(tokenizer.decode(next(data_iter)[0].tolist()))\n",
    "# print(tokenizer.decode([ 1021,   757,   438,   198,   198,    40]))\n",
    "\n",
    "print(\"input shape\",input.shape)\n",
    "print(\"output shape\",output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e905224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# lets do vector embedding\n",
    "\n",
    "vocab_size = 50257\n",
    "dim = 256\n",
    "\n",
    "torch.manual_seed(0)\n",
    "embedding_layer_vec = torch.nn.Embedding(vocab_size, dim)\n",
    "\n",
    "# for all input ids\n",
    "vec_input = embedding_layer_vec(input)\n",
    "print(vec_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25931257",
   "metadata": {},
   "source": [
    "## After vector embeding we will be having for each token id a vector embedding of dimension dim\n",
    "- say we are having a embedding matrix dimension dim = 256 and vocab_size = 50257\n",
    " then what happens is that for each token id we will be having a vector embedding of dimension 256\n",
    " - NOW SAY we have batch size 8 which means in each iteration we have 8 input sequences and 8 output sequences\n",
    " - for each input sequence we will be having a vector embedding of dimension 256\n",
    "now for tokenization we have 8 input sequence with length 4 for each input sequence\n",
    " so we get 8*4 matrix in each batch which becomes 8*4*256 matrix after vector embedding\n",
    " ![vector embedding](./embed.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d3d9615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embed_layer = torch.nn.Embedding(4, dim)# we only need 4 positional embeddings as position are only 4 and is same for same position in different sequence so we just make 4*dim matrix for position embedding\n",
    "\n",
    "# for all input ids\n",
    "pos_input = pos_embed_layer(torch.arange(4))#get position embedding for 0,1,2,3 positions then add them to vector embedding\n",
    "print(pos_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508395c4",
   "metadata": {},
   "source": [
    "## now lets add them together\n",
    "- now we add vector embedding matrix with 8*4*256 matrix and 4*256 matrix as position for each sequence's same position will be same \n",
    "- ![postional embedding](./public/pos.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "491d8ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embedding_input = vec_input + pos_input\n",
    "print(pos_embedding_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786b00a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
