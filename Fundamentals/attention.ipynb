{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b818bfcb",
   "metadata": {},
   "source": [
    "## what is <strong>ATTENTION</strong>\n",
    "in a model to perform well, it should be able to focus on the most relevant parts of the input.which is called <strong>attention</strong>.\n",
    "for example in a sentence like \"A cat sat on the mat and jumped over the dog\", the model should be able to focus on the words \"cat\", \"sat\", \"on\", \"the\", \"mat\" and \"jumped\" and etc. By doing so it makes it understand that the actual sentence is about a cat jumping over dog not a dog jumping over cat or anything else.\n",
    "- it is essential for language translation models to learn how to focus on the most relevant parts of the input.\n",
    "- for exmple we are converting timi sanchai xau,bhai? to english it cant be you fine are,brother? but you are fine,bhai?\n",
    "- here needs attention mechanism.\n",
    "- now in transformers, the model uses attention to focus on the most relevant parts of the input.but before transformers,there used to be RNNs which are neural network + encoder decoder which encoder generated context vector and decoder used that context vector to generate the output.\n",
    "- Attention Mechanism are \n",
    "  1.simplified attention\n",
    "  2.multihead attention\n",
    "  3.self attention\n",
    "\n",
    "## now lets understand RNNs and How they work.\n",
    "1. RNNs are a type of neural network that is used to process sequential data.\n",
    "2. when a single word comes in encoder in encoder a hidden state is generated.\n",
    "3. and it continuously updated until the end of the sentence.\n",
    "4. then in final hidden state there is context of the whole sentence.which is kind of memory of the sentence.\n",
    "5. this final hidden state is used by the decoder to generate the output.\n",
    "6. the output is generated as one word at a time using the context which now helps generating better output.\n",
    "- But there was problem with this that the decoder would use only the final hidden state to generate the output. which could be a problem if the sentence is long.\n",
    "- to solve this problem transformers introduced attention mechanism.\n",
    "\n",
    "# Before Transformers there was Bahdanau attention which would modify the RNNs in which the decoder would use the encoder previous hidden state to generate the output.\n",
    "- for example there is a sentence like \"A cat sat on the mat and jumped over the dog\".\n",
    "  while decoder is translating word jumped,it may give attention to the word cat also whch makes model understand that the actual sentence is about a cat jumping over dog not a dog jumping over cat or anything else.\n",
    "- this is called Bahdanau attention.\n",
    "* upto now we have understood how decoding happens using the encoders previous hidden state to generate the next word.\n",
    "* Lets understand What is self-Attention.\n",
    "  - Self attention is technique in which the mechanism's ability to weigh attention weight within the single input sequence. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0f11e",
   "metadata": {},
   "source": [
    "# simplified Attention mechanism\n",
    "\n",
    "- for example consider we have a sintence that \"Your Journey Starts with a Question\".\n",
    "- after vector and input embedding we get for each word/token a vector embedding a long dimensional vector.\n",
    "- now these vectors while plotting would give us a graph in visual representation.\n",
    "- now to get the context vector for a token with any other token we need to calculate the similarity between the two vectors.\n",
    "- this similarity is given by the dot product of the two vectors.\n",
    "  ![attention](./atten.png)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d96adbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1953abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_calculation(query,input):\n",
    "    # print(input.shape)\n",
    "    attention_scores = torch.empty(len(input))\n",
    "    for i in range(len(input)):\n",
    "        # print(len(input))\n",
    "        attention_scores[i] = torch.dot(query, input[i])\n",
    "    return attention_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8965100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 4])\n",
      "tensor([0.4000, 0.5400, 0.6800, 0.8200, 0.9600, 1.1000])\n"
     ]
    }
   ],
   "source": [
    "# coding in python\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4],\n",
    "        [0.2, 0.3, 0.4, 0.5],\n",
    "        [0.3, 0.4, 0.5, 0.6],\n",
    "        [0.4, 0.5, 0.6, 0.7],\n",
    "        [0.5, 0.6, 0.7, 0.8],\n",
    "        [0.6, 0.7, 0.8, 0.9],\n",
    "    ]\n",
    ")\n",
    "# print(inputs[1])\n",
    "\n",
    "attention_score = attention_calculation(inputs[1],inputs)#calculates attention score with query and all the input tokens but we need to now normalize it to get better results later for use\n",
    "\n",
    "print(attention_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a068e",
   "metadata": {},
   "source": [
    "## normalization makes it between 0 and 1.\n",
    "- one way is to divide each score by the sum of all scores \n",
    "- but we havve problem because we dont want to consider a token attention if it is not near to the query token.\n",
    "- we use softmax to normalize the scores\n",
    "- in this way we get the attention score between 0 and 1 for each token by exponential of atention score divided by sum of all attention scores which make model understand that no need to give attention to a token if it is not near to the query token \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b810854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1142, 0.1313, 0.1510, 0.1737, 0.1999, 0.2299])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def soft_max_naive(x):\n",
    "    return torch.exp(x)/torch.exp(x).sum(dim=0)\n",
    "\n",
    "normalized_attention_score = soft_max_naive(attention_score)\n",
    "print(normalized_attention_score)\n",
    "print(normalized_attention_score.sum(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73ddc0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1142, 0.1313, 0.1510, 0.1737, 0.1999, 0.2299])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#it is better to use pytorch softmax function\n",
    "\n",
    "normalized_attention_score_2 = torch.softmax(attention_score, dim=0)\n",
    "print(normalized_attention_score_2)\n",
    "print(normalized_attention_score_2.sum(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6038c7",
   "metadata": {},
   "source": [
    "# now we want to calculate the context vector\n",
    "- it is calculated by multiplying the normalized attention score with the input tokens's input embedding vector\n",
    "- then after multiplying we get a vector of same dimension as the input embedding vector for each tokens of the input sequence which is context vector for the particular query with tokens of input sequence. \n",
    "- now these context vectors are summed to get the final context vector for the query token.\n",
    "- now the final context vector is used to generate the output by using the decoder.\n",
    "  ![context](./cntxt.png)\n",
    "![attention](./ctxt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6434efec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be5e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2103, 1.0927, 0.5903, 0.6903])\n"
     ]
    }
   ],
   "source": [
    "#code \n",
    "def calculate_context_vector(inputs, normalized_attention_score_2):\n",
    "    context_vector = torch.empty(inputs.shape[1])\n",
    "    for i,x_i in enumerate(inputs):\n",
    "        context_vector+= normalized_attention_score_2[i] * x_i\n",
    "    return context_vector\n",
    "\n",
    "\n",
    "context_vector = calculate_context_vector(inputs, normalized_attention_score_2)\n",
    "print(context_vector)#this is the context vector for the query token 'Journey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7cd2ad64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000],\n",
      "        [0.4000, 0.5400, 0.6800, 0.8200, 0.9600, 1.1000],\n",
      "        [0.5000, 0.6800, 0.8600, 1.0400, 1.2200, 1.4000],\n",
      "        [0.6000, 0.8200, 1.0400, 1.2600, 1.4800, 1.7000],\n",
      "        [0.7000, 0.9600, 1.2200, 1.4800, 1.7400, 2.0000],\n",
      "        [0.8000, 1.1000, 1.4000, 1.7000, 2.0000, 2.3000]])\n",
      "tensor([[0.1279, 0.1414, 0.1562, 0.1727, 0.1908, 0.2109],\n",
      "        [0.1142, 0.1313, 0.1510, 0.1737, 0.1999, 0.2299],\n",
      "        [0.1014, 0.1214, 0.1454, 0.1740, 0.2083, 0.2494],\n",
      "        [0.0897, 0.1118, 0.1393, 0.1735, 0.2163, 0.2695],\n",
      "        [0.0790, 0.1025, 0.1329, 0.1723, 0.2235, 0.2899],\n",
      "        [0.0693, 0.0935, 0.1262, 0.1704, 0.2300, 0.3105]])\n"
     ]
    }
   ],
   "source": [
    "attention = inputs @ inputs.T\n",
    "print(attention)\n",
    "normalized_attention = torch.softmax(attention, dim=-1)\n",
    "print(normalized_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8b2e1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1142, 0.1313, 0.1510, 0.1737, 0.1999, 0.2299])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "row_2 = normalized_attention[1]\n",
    "print(row_2)\n",
    "print(row_2.sum(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "864767b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3790, 0.4790, 0.5790, 0.6790],\n",
      "        [0.3903, 0.4903, 0.5903, 0.6903],\n",
      "        [0.4015, 0.5015, 0.6015, 0.7015],\n",
      "        [0.4123, 0.5123, 0.6123, 0.7123],\n",
      "        [0.4228, 0.5228, 0.6228, 0.7228],\n",
      "        [0.4330, 0.5330, 0.6330, 0.7330]])\n"
     ]
    }
   ],
   "source": [
    "#calculate context vector for each query token\n",
    "context_vector = normalized_attention @ inputs\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6d33d",
   "metadata": {},
   "source": [
    "## How to convert Input Embedding to Context Vector ??\n",
    "- it is done by 1st we calculate the attention score for each token of the input sequence.\n",
    "- this is done by calculating the dot product of the query token with each token of the input sequence.\n",
    "- then we normalize the attention score using softmax.\n",
    "- then we calculate the context vector by 1st multiplying the normalized attention score of each token (with respect to query token) with the input embedding vector of each token.\n",
    "- then we sum all these context vectors to get the final context vector for the query token.\n",
    "- this is fed to the decoder to generate the output.\n",
    "\n",
    "# now how do we even get the attention score for each token of the input sequence and the context vector?\n",
    "- to get attention score with each other in a sequence we do matrix multiplication of input embedding matrix to transpose of input embedding matrix.which returns the attention score for each token of the input sequence with respect to each other token of the input sequence.\n",
    "- now to get the normalized attention score we use softmax.\n",
    "- now to get the context vector for each token of the input sequence we do matrix multiplication of normalized attention score with input embedding matrix.\n",
    "* boom we get the context vector for each token of the input sequence.that 1st row of this vectr gives the context vector for the 1st token and so on... also remember that 1st row of the input embedding matrix gives the input embedding vector for the 1st token.so we just converted all the input embedding vectors to context vectors.\n",
    "\n",
    "** REMEMBER THAT WE NEED TO USE TRAINABLE WEIGHTS FOR THIS. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dcdfb2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
