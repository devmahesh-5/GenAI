{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b818bfcb",
   "metadata": {},
   "source": [
    "## what is <strong>ATTENTION</strong>\n",
    "in a model to perform well, it should be able to focus on the most relevant parts of the input.which is called <strong>attention</strong>.\n",
    "for example in a sentence like \"A cat sat on the mat and jumped over the dog\", the model should be able to focus on the words \"cat\", \"sat\", \"on\", \"the\", \"mat\" and \"jumped\" and etc. By doing so it makes it understand that the actual sentence is about a cat jumping over dog not a dog jumping over cat or anything else.\n",
    "- it is essential for language translation models to learn how to focus on the most relevant parts of the input.\n",
    "- for exmple we are converting timi sanchai xau,bhai? to english it cant be you fine are,brother? but you are fine,bhai?\n",
    "- here needs attention mechanism.\n",
    "- now in transformers, the model uses attention to focus on the most relevant parts of the input.but before transformers,there used to be RNNs which are neural network + encoder decoder which encoder generated context vector and decoder used that context vector to generate the output.\n",
    "- Attention Mechanism are \n",
    "  1.simplified attention\n",
    "  2.multihead attention\n",
    "  3.self attention\n",
    "\n",
    "## now lets understand RNNs and How they work.\n",
    "1. RNNs are a type of neural network that is used to process sequential data.\n",
    "2. when a single word comes in encoder in encoder a hidden state is generated.\n",
    "3. and it continuously updated until the end of the sentence.\n",
    "4. then in final hidden state there is context of the whole sentence.which is kind of memory of the sentence.\n",
    "5. this final hidden state is used by the decoder to generate the output.\n",
    "6. the output is generated as one word at a time using the context which now helps generating better output.\n",
    "- But there was problem with this that the decoder would use only the final hidden state to generate the output. which could be a problem if the sentence is long.\n",
    "- to solve this problem transformers introduced attention mechanism.\n",
    "\n",
    "# Before Transformers there was Bahdanau attention which would modify the RNNs in which the decoder would use the encoder previous hidden state to generate the output.\n",
    "- for example there is a sentence like \"A cat sat on the mat and jumped over the dog\".\n",
    "  while decoder is translating word jumped,it may give attention to the word cat also whch makes model understand that the actual sentence is about a cat jumping over dog not a dog jumping over cat or anything else.\n",
    "- this is called Bahdanau attention.\n",
    "* upto now we have understood how decoding happens using the encoders previous hidden state to generate the next word.\n",
    "* Lets understand What is self-Attention.\n",
    "  - Self attention is technique in which the mechanism's ability to weigh attention weight within the single input sequence. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d0f11e",
   "metadata": {},
   "source": [
    "# simplified Attention mechanism\n",
    "\n",
    "- for example consider we have a sintence that \"Your Journey Starts with a Question\".\n",
    "- after vector and input embedding we get for each word/token a vector embedding a long dimensional vector.\n",
    "- now these vectors while plotting would give us a graph in visual representation.\n",
    "- now to get the context vector for a token with any other token we need to calculate the similarity between the two vectors.\n",
    "- this similarity is given by the dot product of the two vectors.\n",
    "  ![attention](./public/atten.png)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96adbd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m \n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1953abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_calculation(query,input):\n",
    "    # print(input.shape)\n",
    "    attention_scores = torch.empty(len(input))\n",
    "    for i in range(len(input)):\n",
    "        # print(len(input))\n",
    "        attention_scores[i] = torch.dot(query, input[i])\n",
    "    return attention_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8965100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4000, 0.5400, 0.6800, 0.8200, 0.9600, 1.1000])\n"
     ]
    }
   ],
   "source": [
    "# coding in python\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4],\n",
    "        [0.2, 0.3, 0.4, 0.5],\n",
    "        [0.3, 0.4, 0.5, 0.6],\n",
    "        [0.4, 0.5, 0.6, 0.7],\n",
    "        [0.5, 0.6, 0.7, 0.8],\n",
    "        [0.6, 0.7, 0.8, 0.9],\n",
    "    ]\n",
    ")\n",
    "# print(inputs[1])\n",
    "\n",
    "attention_score = attention_calculation(inputs[1],inputs)#calculates attention score with query and all the input tokens but we need to now normalize it to get better results later for use\n",
    "\n",
    "print(attention_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a068e",
   "metadata": {},
   "source": [
    "## normalization makes it between 0 and 1.\n",
    "- one way is to divide each score by the sum of all scores \n",
    "- but we havve problem because we dont want to consider a token attention if it is not near to the query token.\n",
    "- we use softmax to normalize the scores\n",
    "- in this way we get the attention score between 0 and 1 for each token by exponential of atention score divided by sum of all attention scores which make model understand that no need to give attention to a token if it is not near to the query token \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b810854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1142, 0.1313, 0.1510, 0.1737, 0.1999, 0.2299])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def soft_max_naive(x):\n",
    "    return torch.exp(x)/torch.exp(x).sum(dim=0)\n",
    "\n",
    "normalized_attention_score = soft_max_naive(attention_score)\n",
    "print(normalized_attention_score)\n",
    "print(normalized_attention_score.sum(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddc0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1142, 0.1313, 0.1510, 0.1737, 0.1999, 0.2299])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#it is better to use pytorch softmax function\n",
    "\n",
    "normalized_attention_score_2 = torch.softmax(attention_score, dim=0)\n",
    "print(normalized_attention_score_2)\n",
    "print(normalized_attention_score_2.sum(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6038c7",
   "metadata": {},
   "source": [
    "# now we want to calculate the context vector\n",
    "- it is calculated by multiplying the normalized attention score with the input tokens's input embedding vector\n",
    "- then after multiplying we get a vector of same dimension as the input embedding vector for each tokens of the input sequence which is context vector for the particular query with tokens of input sequence. \n",
    "- now these context vectors are summed to get the final context vector for the query token.\n",
    "- now the final context vector is used to generate the output by using the decoder.\n",
    "  ![context](./public/cntxt.png)\n",
    "![attention](./public/ctxt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be5e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3903, 2.3653, 0.5903, 0.6903])\n"
     ]
    }
   ],
   "source": [
    "#code \n",
    "def calculate_context_vector(inputs, normalized_attention_score_2):\n",
    "    context_vector = torch.empty(inputs.shape[1])\n",
    "    for i,x_i in enumerate(inputs):\n",
    "        context_vector+= normalized_attention_score_2[i] * x_i\n",
    "    return context_vector\n",
    "\n",
    "\n",
    "context_vector = calculate_context_vector(inputs, normalized_attention_score_2)\n",
    "print(context_vector)#this is the context vector for the query token 'Journey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd2ad64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000],\n",
      "        [0.4000, 0.5400, 0.6800, 0.8200, 0.9600, 1.1000],\n",
      "        [0.5000, 0.6800, 0.8600, 1.0400, 1.2200, 1.4000],\n",
      "        [0.6000, 0.8200, 1.0400, 1.2600, 1.4800, 1.7000],\n",
      "        [0.7000, 0.9600, 1.2200, 1.4800, 1.7400, 2.0000],\n",
      "        [0.8000, 1.1000, 1.4000, 1.7000, 2.0000, 2.3000]])\n",
      "tensor([[0.1279, 0.1414, 0.1562, 0.1727, 0.1908, 0.2109],\n",
      "        [0.1142, 0.1313, 0.1510, 0.1737, 0.1999, 0.2299],\n",
      "        [0.1014, 0.1214, 0.1454, 0.1740, 0.2083, 0.2494],\n",
      "        [0.0897, 0.1118, 0.1393, 0.1735, 0.2163, 0.2695],\n",
      "        [0.0790, 0.1025, 0.1329, 0.1723, 0.2235, 0.2899],\n",
      "        [0.0693, 0.0935, 0.1262, 0.1704, 0.2300, 0.3105]])\n"
     ]
    }
   ],
   "source": [
    "attention = inputs @ inputs.T\n",
    "print(attention)\n",
    "normalized_attention = torch.softmax(attention, dim=-1)\n",
    "print(normalized_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2e1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1142, 0.1313, 0.1510, 0.1737, 0.1999, 0.2299])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "row_2 = normalized_attention[1]\n",
    "print(row_2)\n",
    "print(row_2.sum(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864767b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3790, 0.4790, 0.5790, 0.6790],\n",
      "        [0.3903, 0.4903, 0.5903, 0.6903],\n",
      "        [0.4015, 0.5015, 0.6015, 0.7015],\n",
      "        [0.4123, 0.5123, 0.6123, 0.7123],\n",
      "        [0.4228, 0.5228, 0.6228, 0.7228],\n",
      "        [0.4330, 0.5330, 0.6330, 0.7330]])\n"
     ]
    }
   ],
   "source": [
    "#calculate context vector for each query token\n",
    "context_vector = normalized_attention @ inputs\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6d33d",
   "metadata": {},
   "source": [
    "## How to convert Input Embedding to Context Vector ??\n",
    "- it is done by 1st we calculate the attention score for each token of the input sequence.\n",
    "- this is done by calculating the dot product of the query token with each token of the input sequence.\n",
    "- then we normalize the attention score using softmax.\n",
    "- then we calculate the context vector by 1st multiplying the normalized attention score of each token (with respect to query token) with the input embedding vector of each token.\n",
    "- then we sum all these context vectors to get the final context vector for the query token.\n",
    "- this is fed to the decoder to generate the output.\n",
    "\n",
    "# now how do we even get the attention score for each token of the input sequence and the context vector?\n",
    "- to get attention score with each other in a sequence we do matrix multiplication of input embedding matrix to transpose of input embedding matrix.which returns the attention score for each token of the input sequence with respect to each other token of the input sequence.\n",
    "- now to get the normalized attention score we use softmax.\n",
    "- now to get the context vector for each token of the input sequence we do matrix multiplication of normalized attention score with input embedding matrix.\n",
    "* boom we get the context vector for each token of the input sequence.that 1st row of this vectr gives the context vector for the 1st token and so on... also remember that 1st row of the input embedding matrix gives the input embedding vector for the 1st token.so we just converted all the input embedding vectors to context vectors.\n",
    "\n",
    "** REMEMBER THAT WE NEED TO USE TRAINABLE WEIGHTS FOR THIS. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dcdfb2",
   "metadata": {},
   "source": [
    "## No we look into the self attention\n",
    "- before we looked into the simplified attention in which we did not use any trainable weights.\n",
    "- trainable weights are used in self attention to provide better results.\n",
    "- here we will have 3 trainable matrices - query matrix, key matrix and value matrix.\n",
    "- we multiply each of these matrix to the input embedding matrix to get the query matrix, key matrix and value matrix.\n",
    "- then we calculate the attention score for each token of the input sequence with respect to each other token of the input sequence.\n",
    "- then we normalize the attention score using softmax.\n",
    "- then\n",
    "\n",
    "![self](./public/self.png)\n",
    "\n",
    "![self2](./public/self_attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding self attention\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4],#your\n",
    "        [0.2, 0.3, 0.4, 0.5],#journey\n",
    "        [0.3, 0.4, 0.5, 0.6],#starts\n",
    "        [0.4, 0.5, 0.6, 0.7],#with\n",
    "        [0.5, 0.6, 0.7, 0.8],#a\n",
    "        [0.6, 0.7, 0.8, 0.9],#question\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f129d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "w_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "w_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "w_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ba17c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8529, 0.6798])\n",
      "tensor([0.9397, 0.5642])\n",
      "tensor([0.8143, 0.7649])\n"
     ]
    }
   ],
   "source": [
    "v_key = x_2 @ w_key\n",
    "v_query = x_2 @ w_query\n",
    "v_value = x_2 @ w_value\n",
    "print(v_key)\n",
    "print(v_query)\n",
    "print(v_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 2])\n",
      "torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ w_key\n",
    "queries = inputs @ w_query\n",
    "values = inputs @ w_value\n",
    "print(keys.shape)\n",
    "print(queries.shape)\n",
    "print(values.shape)\n",
    "# print(keys)\n",
    "# print(queries)\n",
    "# print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38510915",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_score_2 = queries @ keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32134ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6168, 0.8560, 1.0952, 1.3344, 1.5736, 1.8128],\n",
      "        [0.8542, 1.1850, 1.5158, 1.8466, 2.1775, 2.5083],\n",
      "        [1.0916, 1.5140, 1.9365, 2.3589, 2.7813, 3.2038],\n",
      "        [1.3290, 1.8430, 2.3571, 2.8712, 3.3852, 3.8993],\n",
      "        [1.5664, 2.1720, 2.7777, 3.3834, 3.9891, 4.5948],\n",
      "        [1.8037, 2.5010, 3.1984, 3.8957, 4.5930, 5.2903]])\n",
      "tensor([[0.1048, 0.1241, 0.1469, 0.1740, 0.2061, 0.2441],\n",
      "        [0.0859, 0.1085, 0.1371, 0.1732, 0.2188, 0.2765],\n",
      "        [0.0696, 0.0938, 0.1265, 0.1705, 0.2298, 0.3098],\n",
      "        [0.0558, 0.0803, 0.1155, 0.1661, 0.2389, 0.3436],\n",
      "        [0.0443, 0.0680, 0.1044, 0.1602, 0.2458, 0.3773],\n",
      "        [0.0349, 0.0571, 0.0935, 0.1531, 0.2507, 0.4106]])\n",
      "tensor([[1.2600, 1.2205],\n",
      "        [1.2994, 1.2608],\n",
      "        [1.3368, 1.2990],\n",
      "        [1.3718, 1.3348],\n",
      "        [1.4042, 1.3679],\n",
      "        [1.4339, 1.3983]])\n"
     ]
    }
   ],
   "source": [
    "#now we will use these 3 matrices instead of the input embeddings to calculate the attention score\n",
    "\n",
    "#to get the attention score for 2nd token \"Journey\" we will dot product the query vectors 2nd row with all the key vectors\n",
    "\n",
    "\n",
    "attention_score = queries @ keys.T\n",
    "print(attention_score)\n",
    "d_k = keys.shape[-1]\n",
    "normalized_attention_score = torch.softmax(attention_score/d_k**0.5, dim=-1)#across the columns of the matrix\n",
    "print(normalized_attention_score)\n",
    "context_vector = normalized_attention_score @ values\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37928884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7303, 1.3201],\n",
      "        [0.7526, 1.3610],\n",
      "        [0.7739, 1.3998],\n",
      "        [0.7939, 1.4363],\n",
      "        [0.8125, 1.4703],\n",
      "        [0.8297, 1.5016]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#lets now impliment context vector from input embeddings using nn.modules\n",
    "\n",
    "class selfAttention_v1(torch.nn.Module):\n",
    "    def __init__(self, input_dim, context_dim):\n",
    "        super().__init__()\n",
    "        self.w_query = torch.nn.Parameter(torch.rand(input_dim, context_dim))\n",
    "        self.w_key = torch.nn.Parameter(torch.rand(input_dim, context_dim))\n",
    "        self.w_value = torch.nn.Parameter(torch.rand(input_dim, context_dim))\n",
    "    \n",
    "    def forward (self,input):\n",
    "        keys = input @ self.w_key\n",
    "        queries = input @ self.w_query\n",
    "        values = input @ self.w_value\n",
    "        attention_score = queries @ keys.T\n",
    "        d_k = keys.shape[-1]\n",
    "        normalized_attention_score = torch.softmax(attention_score/d_k**0.5, dim=-1)\n",
    "        context_vector = normalized_attention_score @ values\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "model = selfAttention_v1(inputs.shape[-1], 2)#input columns and parameter's rows should be same as we will be performing matrix multiplication\n",
    "context_vector = model(inputs)\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6a614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1236,  0.4347],\n",
      "        [-0.1236,  0.4343],\n",
      "        [-0.1236,  0.4339],\n",
      "        [-0.1235,  0.4334],\n",
      "        [-0.1235,  0.4330],\n",
      "        [-0.1235,  0.4325]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#lets now impliment context vector from input embeddings using nn.modules\n",
    "\n",
    "class selfAttention_v2(torch.nn.Module):\n",
    "    def __init__(self, input_dim, context_dim):\n",
    "        super().__init__()\n",
    "        self.w_query = torch.nn.Linear(input_dim, context_dim, bias=False)\n",
    "        self.w_key = torch.nn.Linear(input_dim, context_dim, bias=False)\n",
    "        self.w_value = torch.nn.Linear(input_dim, context_dim, bias=False)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # call the Linear modules instead of trying to matrix-multiply them\n",
    "        keys = self.w_key(inputs)\n",
    "        queries = self.w_query(inputs)\n",
    "        values = self.w_value(inputs)\n",
    "        attention_score = queries @ keys.T\n",
    "        d_k = keys.shape[-1]\n",
    "        normalized_attention_score = torch.softmax(attention_score / d_k**0.5, dim=-1)\n",
    "        context_vector = normalized_attention_score @ values\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = selfAttention_v2(inputs.shape[-1], 2)#input columns and parameter's rows should be same as we will be performing matrix multiplication\n",
    "context_vector = model(inputs)\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b407637",
   "metadata": {},
   "source": [
    "## Now the causal mechanism\n",
    "- it is mechanism in which the attention of the token before the token considered is only taken \n",
    "- it makes the attention matrix a lower triangular matrix and then normalized the matrix\n",
    "# steps to follow:\n",
    " 1. 1st we will get the input embedding matrix for the tokens as earlier\n",
    " 2. then we get the query, key and value weight with torch.nn.Parameterwith input_dim = input embedding matrix's columns and wanted context vector's columns\n",
    " 3. we matrix multiply each of these matrices to the input embedding matrix to get the query matrix, key matrix and value matrix.\n",
    " 4. then we get the attention score for each token of the input sequence with respect to each other token of the input sequence by matrix multiplication of query matrix to transpose of key matrix.\n",
    " 5. after this we will mask the attention score to avoid data leakages by setting the upper triangular entries to -inf and then mask the attention score wwith this upper matrix.\n",
    " 6. now to get the normalized attention score we use softmax to attention score/d_k**0.5 where d_k is the number of columns of the key matrix.after that we may use nn.Dropout to avoid overfitting.\n",
    " 7. now to get the context vector for each token of the input sequence we do matrix multiplication of normalized attention score with value matrix.\n",
    "\n",
    "![causal](./public/causal.png)\n",
    "\n",
    "# Dropout\n",
    "![causal](./public/dropout.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a41efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = attention_score.shape[0]\n",
    "masked = torch.tril(torch.ones(context_len,context_len))\n",
    "attention_masked = masked * attention_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf5fad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4189, 0.5811, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2403, 0.3333, 0.4263, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1582, 0.2194, 0.2806, 0.3418, 0.0000, 0.0000],\n",
      "        [0.1128, 0.1564, 0.2000, 0.2436, 0.2872, 0.0000],\n",
      "        [0.0848, 0.1175, 0.1503, 0.1830, 0.2158, 0.2486]])\n"
     ]
    }
   ],
   "source": [
    "total = attention_masked.sum(dim=1,keepdim=True)\n",
    "normalized_attention_masked = attention_masked/total\n",
    "print(normalized_attention_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2852013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6168,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.8542, 1.1850,   -inf,   -inf,   -inf,   -inf],\n",
      "        [1.0916, 1.5140, 1.9365,   -inf,   -inf,   -inf],\n",
      "        [1.3290, 1.8430, 2.3571, 2.8712,   -inf,   -inf],\n",
      "        [1.5664, 2.1720, 2.7777, 3.3834, 3.9891,   -inf],\n",
      "        [1.8037, 2.5010, 3.1984, 3.8957, 4.5930, 5.2903]])\n"
     ]
    }
   ],
   "source": [
    "# ensure no present token is influenced by future tokens;this avoids the data leakages\n",
    "# before softmax set the upper-triangular entries to -inf\n",
    "mask = torch.triu(torch.ones(context_len, context_len), diagonal=1).bool()\n",
    "masked = attention_score.masked_fill(mask, float(\"-inf\"))\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a4f6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4418, 0.5582, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2401, 0.3236, 0.4363, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1336, 0.1922, 0.2765, 0.3977, 0.0000, 0.0000],\n",
      "        [0.0712, 0.1092, 0.1676, 0.2572, 0.3948, 0.0000],\n",
      "        [0.0349, 0.0571, 0.0935, 0.1531, 0.2507, 0.4106]])\n"
     ]
    }
   ],
   "source": [
    "normalized_masked_attention = torch.softmax(masked/keys.shape[-1]**0.5,dim=-1)\n",
    "print(normalized_masked_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f9c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4807, 0.6667, 0.8527, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4388, 0.5612, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3128, 0.0000, 0.4872, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2350, 0.3006, 0.3661, 0.4316, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# now we apply dropout so to avooid overfitting \n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "dropout_attn = dropout(normalized_attention_masked)\n",
    "print(dropout_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136af11",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#now causal attention class\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCausal_attention\u001b[39;00m(\u001b[43mtorch\u001b[49m.nn.Module):\n\u001b[32m      3\u001b[39m      \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim, context_dim,qkv_bias=\u001b[38;5;28;01mFalse\u001b[39;00m, attention_len=\u001b[32m6\u001b[39m):\n\u001b[32m      4\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#now causal attention class\n",
    "class Causal_attention(torch.nn.Module):\n",
    "     def __init__(self, input_dim, context_dim,qkv_bias=False, attention_len=6):\n",
    "        super().__init__()\n",
    "        self.w_query = torch.nn.Linear(input_dim, context_dim, bias=qkv_bias)\n",
    "        self.w_key = torch.nn.Linear(input_dim, context_dim, bias=qkv_bias)\n",
    "        self.w_value = torch.nn.Linear(input_dim, context_dim, bias=qkv_bias)\n",
    "        self.register_buffer(\"mask\",torch.triu(torch.ones(attention_len,attention_len),diagonal=1))\n",
    "        \n",
    "     def forward(self, inputs):\n",
    "      b, num_token_seq, token_dim = inputs.shape\n",
    "      keys = self.w_key(inputs)\n",
    "      queries = self.w_query(inputs)\n",
    "      values = self.w_value(inputs)\n",
    "\n",
    "      attention_score = queries @ keys.transpose(1,2)\n",
    "      masked = attention_score.masked_fill(self.mask[:num_token_seq, :num_token_seq].bool(), float('-inf'))\n",
    "      \n",
    "      normalized_attention_score = torch.softmax(masked / (keys.shape[-1]**0.5), dim=-1)\n",
    "      dropout = torch.nn.Dropout(0.0)\n",
    "      attention = dropout(normalized_attention_score)\n",
    "      \n",
    "      context_vector = attention @ values\n",
    "      return context_vector\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e76e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2252,  0.2267, -0.3400],\n",
      "         [-0.2489,  0.2630, -0.3248],\n",
      "         [-0.2724,  0.2989, -0.3097],\n",
      "         [-0.2957,  0.3344, -0.2948],\n",
      "         [-0.3188,  0.3697, -0.2800],\n",
      "         [-0.3417,  0.4047, -0.2653]],\n",
      "\n",
      "        [[-0.2252,  0.2267, -0.3400],\n",
      "         [-0.2489,  0.2630, -0.3248],\n",
      "         [-0.2724,  0.2989, -0.3097],\n",
      "         [-0.2957,  0.3344, -0.2948],\n",
      "         [-0.3188,  0.3697, -0.2800],\n",
      "         [-0.3417,  0.4047, -0.2653]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs,inputs),dim=0\n",
    ")\n",
    "print(batch)\n",
    "\n",
    "# pass attention_len as a keyword (positional args cannot come after keyword args)\n",
    "model = Causal_attention(batch.shape[-1], 3, qkv_bias=True, attention_len=batch.shape[1])\n",
    "context_vector = model(batch)\n",
    "print(context_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc9aa5",
   "metadata": {},
   "source": [
    "In practical terms, implementing multi-head attention involves creating multiple instances of the self-attention mechanism, each with its own weights, and then combining their outputs\n",
    "\n",
    "In code, we can achieve this by implementing a simple MultiHeadAttentionWrapper class that stacks multiple instances of our previously implemented CausalAttention module:\n",
    "\n",
    "In multihead attention we just create multiple key,query and value matrixes and then perform matrix multiplication with each of these matrixes to get the query matrix, key matrix and value matrix.\n",
    "\n",
    "then find context vector for each pairs these \n",
    "at last we just add these context vectors along columns to get the final output context vector.\n",
    "\n",
    "![multihead](./public/multi.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32bd6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ModuleList(\n",
    "            [Causal_attention(d_in, d_out, qkv_bias, attention_len=context_length) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba487be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "tensor([[[ 0.3326,  0.5659, -0.3132,  0.0752,  0.4566,  0.2729],\n",
      "         [ 0.3456,  0.5650, -0.2237,  0.0313,  0.5977,  0.3053],\n",
      "         [ 0.3440,  0.5604, -0.2000,  0.0178,  0.6413,  0.3138],\n",
      "         [ 0.3103,  0.4941, -0.1606,  0.0089,  0.5729,  0.2785],\n",
      "         [ 0.2430,  0.4287, -0.1643,  0.0071,  0.5566,  0.2514],\n",
      "         [ 0.2648,  0.4316, -0.1375,  0.0023,  0.5363,  0.2508]],\n",
      "\n",
      "        [[ 0.3326,  0.5659, -0.3132,  0.0752,  0.4566,  0.2729],\n",
      "         [ 0.3456,  0.5650, -0.2237,  0.0313,  0.5977,  0.3053],\n",
      "         [ 0.3440,  0.5604, -0.2000,  0.0178,  0.6413,  0.3138],\n",
      "         [ 0.3103,  0.4941, -0.1606,  0.0089,  0.5729,  0.2785],\n",
      "         [ 0.2430,  0.4287, -0.1643,  0.0071,  0.5566,  0.2514],\n",
      "         [ 0.2648,  0.4316, -0.1375,  0.0023,  0.5363,  0.2508]]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) \n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens = 6\n",
    "d_in, d_out = inputs.shape[-1], 3\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe411ce",
   "metadata": {},
   "source": [
    "For example, if we use this MultiHeadAttentionWrapper class with two attention heads (via num_heads=2) and CausalAttention output dimension d_out=2, this results in a 4- dimensional context vectors (d_out*num_heads=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae6d6b",
   "metadata": {},
   "source": [
    "### IMPLEMENTING MULTI-HEAD ATTENTION WITH WEIGHT SPLITS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d2baf9",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "Instead of maintaining two separate classes, MultiHeadAttentionWrapper and\n",
    "CausalAttention, we can combine both of these concepts into a single\n",
    "MultiHeadAttention class. \n",
    "\n",
    "Also, in addition to just merging the\n",
    "MultiHeadAttentionWrapper with the CausalAttention code, we will make some other\n",
    "modifications to implement multi-head attention more efficiently.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42458cdd",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "In the MultiHeadAttentionWrapper, multiple heads are implemented by creating a list\n",
    "of CausalAttention objects (self.heads), each representing a separate attention head.\n",
    "\n",
    "\n",
    "The CausalAttention class independently performs the attention mechanism, and the\n",
    "results from each head are concatenated.\n",
    "\n",
    "In contrast, the following MultiHeadAttention\n",
    "class integrates the multi-head functionality within a single class. \n",
    "\n",
    "\n",
    "It splits the input into\n",
    "multiple heads by reshaping the projected query, key, and value tensors and then combines\n",
    "the results from these heads after computing attention.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07efa1f9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Step 1: Reduce the projection dim to match desired output dim\n",
    "\n",
    "Step 2: Use a Linear layer to combine head outputs\n",
    "\n",
    "Step 3: Tensor shape: (b, num_tokens, d_out)\n",
    "\n",
    "Step 4: We implicitly split the matrix by adding a `num_heads` dimension. Then we unroll last dim: (b,\n",
    "num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "\n",
    "Step 5: Transpose from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads, num_tokens, head_dim)\n",
    "\n",
    "Step 6: Compute dot product for each head\n",
    "\n",
    "Step 7: Mask truncated to the number of tokens\n",
    "\n",
    "Step 8: Use the mask to fill attention scores\n",
    "\n",
    "Step 9: Tensor shape: (b, num_tokens, n_heads, head_dim)\n",
    "\n",
    "Step 10: Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "\n",
    "Step 11: Add an optional linear projection\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3889a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # assert (d_out % num_heads == 0), \n",
    "        #     \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = torch.nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b014a9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
      "\n",
      "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
      "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the tensor with 3 rows and 6 columns\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1 = token1\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) \n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 6\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9a821",
   "metadata": {},
   "source": [
    "## Lets dive deepper into the LLM architecture\n",
    "we will implement the GPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4daf3b4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
